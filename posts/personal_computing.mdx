---
title: "What is the future of personal computing?"
date: "2025-08-25"
publish: false
---

I've been thinking a lot about the future of personal computing. If you think about it, it hasn't really changed all that much in the last 20 years. Sure, the devices have gotten faster, smaller, thinner and in some cases smarter, but generally speaking the form factor and way we use the devices hasn't really changed.

In general, the way that we use our personal computing devices is:

1. You look at screen
2. You type command
3. Computer does something while you watch
4. You reason if it did the right thing and then respond accordingly

This has had massive consequences. Because we haven't had another way of interacting with these devices, social media companies, app developers and others have been able to hijack our eyes and brains. We've been sent on a dopamine rollercoaster for the last 20 years that has

I want to pose a question: would facebook have become what it has become if we could tell our devices to do stuff and it reliably did that stuff? Was it inevitable that we would have wanted to prefer to do things with our eyes and hands instead of our voice? I mean there are valid use-cases for that. You don't always want to dictate to your phone. Whether it's a private message or something else, somethings are left better unsaid but typed.

I think what im trying to get at here is, if we changed the way that we mainly interact with our devices what happens to many companies that rely on that method of interaction?

But does it have to be this way? _Should_ it be this way?

Now that we have increasingly capable AI agents and devices that can just do stuff, do we even reall need to look at our phones anymore? Or can we mainly dictate our agents to do stuff for us? Let's run with this for a minute and see what kind of world we live in.

First, I think technology generally is becoming more ambient. It's more merging with the human form factor. For example, most gen z's wear their airpods all day long even if nothing is playing. You can imagine that airpod, or more generally, audio device, becomes a more permanant part of the human body.

I wear my apple watc pretty mcuh all day. If it had the battery life, i would never take it off. I can see a future where peripherals and wearables are largly the way that we interact with our agents and software. If we have to carry a rectangular device in our pockets it might not even have a pocket. It might just be an energy and compute bank for our wearbles.

i think the missing piece here is some sort of vision device. Whether thats AR glasses or something else, its clear that is requried as well. The combination of awatch, audio device and vision device completes the trifecta. We can now see things in AR/VR through our glasses (texts, messages, routes, etc. ), we can hear things (phone calls, audio) and we can track things (biomechanics, time).

But what there's more. With advacnements in BCI, we can **control** things. Its pretty amazing what you can do with you brain. I can think of a command and my technology can just do that by reading and decoding my EEG waves. This isn't science fiction, this is reality today.

I dont even need ot use my hands or voice in some cases to do things. i can think of a command and my EEG sensor can decode that brainwave, map it to a call command, and then ask you, who do you want to call, you think of the person's face or name, decode that, then it calls.

So why do we need a screen at all? i think ultimately, we won't need it save for a few things but moreso in the transition period from screens to no screens, people will want it.

From a technical perspective, this vision isn't that far away. We can do a lot of this today, albeit not perfectly, but it's clear to me that these thigns will only improve.
