---
title: "From matmul to ChatGPT"
date: "2025-10-04"
publish: false
---

import Note from "../components/Note";

Over the past few months, I've built two LLMs from scratch, namely GPT-rs and llama2.rs. These are full re-writes of GPT-2 from Python to Rust and Llama2 from C to Rust. At this point, I feel reasonably comfortable with the transformer architecture and how it works.

One of the core operations within the layers of a neural net is matrix multiplication or `matmul` as it's often referred to. This `matmul` operation happens trillions of times within large langauge models (LLMs) like ChatGPT and is one the main reasons why ChatGPT is able to provide coherent (usually!) responses.

But I often find myself asking, _how_ does multiplying two matrices together lead to a system like ChatGPT giving us a coherent response to our prompts?

It's one thing to implement `matmul` it's another to deeply understand why it works.

Like with many of my blogs, I start at the beginning and then build to the intuition. We'll first go through the basics of matrix multiplcation and then cover neural nets and then lastly bring them together.

# Vector Multiplication

Matrix multiplication is a way to linearly combine two vectors in order to create a new vector that is the weighted sum of the two component vectors.

Let's break down exactly what that means.

A **vector** is a direction in space that has a magnitude. We use bracket notation to represent the vector in the form `[x,y]`, where `x` and `y` are dimensions in space. For example, the vector, $v_1$, at `[0,1]` can be drawn on a coordinate plane like:

![ve](/whymatmul/vec1.png)

<Note>

Throughout this post, we'll write vectors as `[x, y]` for readability, but they formally represent column vectors $\begin{bmatrix} x \\ y \end{bmatrix}$.

</Note>

The vector starts at the origin point of `[0,0]` and goes to `[0,1]`. Since it only has two dimensions (`x` and `y`), we can easily draw it on a 2-dimensional coordinate plane. However, in machine learning, we often use vectors with hundreds or thousands of dimensions. As humans, we can't visualize this many dimensions but computers can easily work with them.

<Note>

Why so many dimensions? Because language is complex! A word might need dimensions for:

- Sentiment (positive/negative)
- Formality (casual/formal)
- Tense (past/present/future)
- Category (animal/object/action)
- And hundreds more nuanced aspects of meaning

This is one of the key parameters that engineers tune when building models. Too many dimensions and the model becomes slow and prone to overfitting. Too few dimensions and it can't capture language's complexity. Modern LLMs typically use between 768 and 4096 dimensions per word.

</Note>

The **direction** of the vector is the direction of the arrowhead and usually is written as the angle the vector makes with regard to the positive x-axis or a compass direction. Since our vector points straight up, it has a direction of 90 degrees.

The **magnitude** of the vector is the numerical size or length of the vector. Our vector has a magnitude of `1` since it starts at `[0,0]` and ends at `[0,1]`.

I can then add another vector, $v_2$, here at `[1,0]` shown in orange:

![ve](/whymatmul/vec2.png)

These two vectors, `[0,1]` and `[1,0]`, are also called **basis vectors**. So if we want to linearly combine these vectors, we simply take the weighted sum of the vectors, which we can represent using the equation:

$$
a v_1 + b v_2
$$

Where $a$ and $b$ are weights (scalars) or $[a,b]$ is a **weight vector**. If we randomly set these weights to $a = 0.5$, $b = 0.8$ and plug them into our equation, we get:

$$
a v_1 + b v_2
$$

$$
0.5 \cdot [0,1] + 0.8 \cdot [1,0]
$$

We can now multiply each element in the $v_1$ `[0,1]` vector by $0.5$:

$$
a \cdot v_1 = 0.5 \cdot [0, 1] = [0.5 \cdot 0, 0.5 \cdot 1] = [0, 0.5]
$$

So we took our original `[0,1]` basis vector and then multiplied it by the $x$ element in our weight vector $0.5$ and got `[0,0.5]`. We just shrunk our vector or reduced it's magnitude by 50%! This is the heart of linear algebra. It's about stretching, scaling and rotating vectors in some dimensional space.

We can also do the same thing for the $v_2$ `[1,0]` vector and get:

$$
a \cdot v_2 = 0.8 \cdot [1, 0] = [0.8 \cdot 1, 0.8 \cdot 0] = [0.8,0]
$$

This time, we only shrunk our vector in the x-direction by 20%.

Lastly, we can add these two vectors by adding the `x` position of each vector and then the `y` position of each vector like this:

$$
\begin{align}
a v_1 + b v_2 &= [0, 0.5] + [0.8,0]\\
&= [0 + 0.5, 0.8 + 0]\\
&= [0.5, 0.8]
\end{align}
$$

This creates a new vector that we can plot along side the original two basis vectors:

![vec](/whymatmul/res_vec.png)

So we've now taken the weighted sum of the two input vectors, $v_1$ and $v_2$ and created a new vector which is a combination of the two input vectors.

But what does this actually mean? How do we move from vectors to matrices?

# From Vectors to Matrices

Matrices are just a collection of column transformed basis vectors. For example, let's look at this matrix:

$$
M =
\begin{bmatrix}
2 & 1 \\
1 & 3
\end{bmatrix}
$$

Think of this as saying:

- The **first column** ([2,1]) tells us where the (x)-axis “basis vector” ([1,0]) gets mapped in space.
- The **second column** ([1,3]) tells us where the (y)-axis “basis vector” ([0,1]) gets mapped in space.

Visually, this looks like:

![vec](/whymatmul/matrix1.png)

Now we can clearly see that the original x-axis basis vector,`[1,0]`, has been transformed to `[2,1]` and the original y-axis basis vector, `[0,1]`, has been transformed to `[1,3]`. The matrix stores the transformed basis vectors which exactly matches our definition of a matrix from above!

Lastly, if you wanted to multiply this matrix by an input vector, $\mathbf{v} = [x,y] = [2,3]$, you can just do the same thing we did above with the input vectors by taking each column of the matrix and multiplying it by the element in the same index in the vector. Let's walk through it step-by-step:

$$
M = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad \mathbf{v} = [2,3]
$$

We multiply each **column** of $M$ by the corresponding element in $\mathbf{v}$:

$$
M \cdot \mathbf{v} = 2 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} + 3 \cdot \begin{bmatrix} 1 \\ 3 \end{bmatrix}
$$

**Step 1: Scale the first column by 2**

$$
2 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 4 \\ 2 \end{bmatrix}
$$

**Step 2: Scale the second column by 3**

$$
3 \cdot \begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 3 \\ 9 \end{bmatrix}
$$

**Step 3: Add them together to get the result vector**

$$
\begin{bmatrix} 4 \\ 2 \end{bmatrix} + \begin{bmatrix} 3 \\ 9 \end{bmatrix} = \begin{bmatrix} 7 \\ 11 \end{bmatrix}
$$

**Final Answer**

$$
M \cdot \mathbf{v} = \begin{bmatrix} 7 \\ 11 \end{bmatrix}
$$

And there we have matrix multiplication! Let's look at this visually:

![vec](/whymatmul/mm.png)

There are a few vectors here so let's go through them:

1. The solid blue vector is the input vector of $[2,3]$. We can graph this in our coordinate plane starting from the origin $[0,0]$ and extending to $[2,3]$.
2. The solid green vector is the 1st column of the matrix $M = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, which is $\begin{bmatrix} 2 \\ 1 \end{bmatrix}$
3. The dashed green vector is the 1st element of the input vector multiplied by the 1st column of the matrix, $2 \cdot [2,1] = [4,2]$
4. The solid orange vector is the 2st column of the matrix $M = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, which is $\begin{bmatrix} 1 \\ 3 \end{bmatrix}$.
5. The dashed orange vector is the 2st element of the input vector multiplied by the 2st column of the matrix, $3 \cdot [1,3] = [3,9]$
6. The solid red vector is resulting vector when we add the dashed orange vector and the dashed green vector.

But wait! Why do we add the two vectors together at the end? I thought the matrix was a collection of two vectors, so wouldn't we expect to have two vectors in the final result?

This is an excellent question and gets at the heart of matrix multiplication.

When you multiply a **matrix** by a **vector**, you get a **vector**.

You're not combining two vectors to make a matrix, you're using the input vector to tell you "how much" of each column, in the matrix, to take, and then you sum them up into a single result vector.

Think of it this way:

- The matrix $M$ has two "basis directions" (its columns)s
- The input vector $[2, 3]$ says "give me **2 units** of the first direction and **3 units** of the second direction"
- You add those scaled directions together to get the result vector

You're not keeping the columns separate, you're **combining** them according to the weights in your input vector!

So a `matmul` is essentially: _"take your input vector, express it as a combination of basis vectors, and then remap those bases into new directions as dictated by the matrix."_

# From Matrices to Meaning

At first glance, this matrix multiplication might seem like simple geometry. We're scaling and stretching and, in some cases, rotating vectors but how does this eventually get us answers in ChatGPT? Here's where it gets interesting: what if these vectors don't represent directions in space, but instead represent meaning in some abstract space?

In a large language model like ChatGPT, every word is represented as a vector, typically with hundreds or thousands of dimensions instead of just 2 that we saw above. When the model sees the word "king", it's not just storing the letters k-i-n-g, it's storing a vector like `[0.23, -0.45, 0.87, ...]` that captures the meaning of "king" based on how it's used in language. It learns this meaning as it's being trained on every book, article and piece of data on the internet and at the end of that training spits out a vector like `[0.23, -0.45, 0.87, ...]`.

But how does it capture the meaning of words like "king"? I thought you'd never ask. Let's look at a simple example.

Imagine we have simple 2D "word vectors" where:

- The **first dimension** represents "royalty" (0 = common, 1 = royal)
- The **second dimension** represents "femininity" (0 = masculine, 1 = feminine)

For example:

- "king" → `[0.9, 0.1]` (high royalty, low femininity)
- "queen" → `[0.9, 0.9]` (high royalty, high femininity)
- "man" → `[0.1, 0.1]` (low royalty, low femininity)
- "woman" → `[0.1, 0.9]` (low royalty, high femininity)

Now if we do the some simple vector arithmetic: `king - man + woman`:

$$
[0.9, 0.1] - [0.0, 0.1] + [0.0, 0.9] = [0.9, 0.9]
$$

This result is `[0.9, 0.9]` which matches "queen"!

**Why does this work?** We:

1. Started with "king" (royal + masculine)
2. Subtracted "man" (removing the masculine aspect)
3. Added "woman" (adding the feminine aspect)
4. Ended up with a royal + feminine concept = "queen"

We've done the same weighted sum operation, except now it's operating on **semantic meaning** rather than spatial directions. We've taken our first step from matrix multiplication to meaning.

As we mentioned above, LLMs like ChatGPT have billions (maybe trillions!) of weights that get refined during training. Through matrix multiplications, the model discovers that words used in similar contexts should have similar vectors.

So how does a neural network actually learn these meaningful representations?

This is where neural networks come into play.

# Learning Meaning

Let's look at a simple neural net to build the intuition.

![nn](/whymatmul/nn.png)
