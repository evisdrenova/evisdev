---
title: "From matmul to ChatGPT"
date: "2025-10-04"
publish: false
---

import Note from "../components/Note";

Over the past few months, I've built two LLMs from scratch, namely GPT-rs and llama2.rs. At this point, I feel reasonably comfortable with the transformer architecture and how it works.

One of the core operations within the layers of a neural net is matrix multiplication or `matmul` as it's often referred to. This `matmul` operation happens trillions of times within large langauge models (LLMs) like ChatGPT and is one the main reasons why ChatGPT is able to provide coherent (usually!) responses.

But I often find myself asking, _how_ and _why_ does multiplying matrices trillions of times lead to ChatGPT giving us a coherent response to our prompts?

It's one thing to implement `matmul` it's another to deeply understand why it works.

Let's start at the beginning and build our way to the intuition.

# Vector Multiplication

The most fundamental component of matrix multiplication is the **vector**. A **vector** is a direction in space that has a magnitude. We use bracket notation to represent the vector in the form `[x,y]`, where `x` and `y` are dimensions in space.

<Note>

Throughout this post, I write vectors as `[x, y]` for readability, but they formally represent column vectors $\begin{bmatrix} x \\ y \end{bmatrix}$.

</Note>

For example, we can draw the vector, $v_1$, at `[0,1]` on a coordinate plane:

![ve](/whymatmul/vec1.png)

The vector starts at the origin point of `[0,0]` and goes to `[0,1]`. Since it only has two dimensions (`x` and `y`), we can easily draw it on a 2-dimensional coordinate plane. However, in machine learning, we often use vectors with hundreds or thousands of dimensions. As humans, we can't visualize this many dimensions but computers can easily work with them.

<Note>

Why do we use so many dimensions? Because language is complex! Every dimension can represent a "feature" of a word. So a word might need dimensions for:

- Sentiment (positive/negative)
- Formality (casual/formal)
- Tense (past/present/future)
- Category (animal/object/action)
- And hundreds more nuanced aspects of meaning

This is one of the key parameters that engineers tune when building models. Too many dimensions and the model becomes slow and prone to overfitting. Too few dimensions and it can't capture language's complexity. Modern LLMs typically use between 768 and 4096 dimensions per word.

</Note>

The **direction** of the vector is the direction of the arrowhead and usually is written as the angle the vector makes with regard to the positive x-axis or a compass direction. Since our vector points straight up, it has a direction of 90 degrees relative to the x-axis.

The **magnitude** of the vector is the numerical size or length of the vector. Our vector has a magnitude of `1` since it starts at `[0,0]` and ends at `[0,1]`.

I can add another vector, $v_2$, here at `[1,0]` shown in orange:

![ve](/whymatmul/vec2.png)

These two vectors, `[0,1]` and `[1,0]`, are also called **basis vectors**. So if we want to combine these vectors, we take the weighted sum of the vectors, which we can represent using the equation:

$$
a v_1 + b v_2
$$

Where $a$ and $b$ are weights or $[a,b]$ is a **weight vector** (we'll see an example of this below). If we randomly set these weights to $a = 0.5$, $b = 0.8$ and plug them into our equation, we get:

$$
a v_1 + b v_2
$$

$$
0.5 \cdot [0,1] + 0.8 \cdot [1,0]
$$

We can now multiply each element in the $v_1$ vector by $0.5$:

$$
a \cdot v_1 = 0.5 \cdot [0, 1] = [0.5 \cdot 0, 0.5 \cdot 1] = [0, 0.5]
$$

So we took our original `[0,1]` basis vector and then multiplied it by the $x$ element in our weight vector and got `[0,0.5]`. We just shrunk our vector or, more precisely, reduced it's magnitude by 50%! This is the heart of linear algebra. It's about stretching, scaling and rotating vectors in some dimensional space.

We can also do the same thing for the $v_2$ `[1,0]` vector and get:

$$
a \cdot v_2 = 0.8 \cdot [1, 0] = [0.8 \cdot 1, 0.8 \cdot 0] = [0.8,0]
$$

This time, we only shrunk our vector in the x-direction by 20%.

Lastly, we can add these two vectors by adding the `x` position of each vector and then the `y` position of each vector like this:

$$
\begin{align}
a v_1 + b v_2 &= [0, 0.5] + [0.8,0]\\
&= [0 + 0.5, 0.8 + 0]\\
&= [0.5, 0.8]
\end{align}
$$

This creates a new vector that we can plot along side the original two basis vectors:

![vec](/whymatmul/res_vec.png)

So we've now taken the weighted sum of the two input vectors, $v_1$ and $v_2$ and created a new vector which is the linear combination of the two input vectors.

But what does this actually mean? How do we move from vectors to matrices?

# From Vectors to Matrices

Matrices are collections of basis vectors (as columns) that have been transformed by some weights. For example, if we take our basis vectors from above,`[0,1]` and `[1,0]`, we can transform them with:

$$
M =
\begin{bmatrix}
2 & 1 \\
1 & 3
\end{bmatrix}
$$

This tells us that:

- The **x-axis basis vector** `[1,0]` is mapped to `[2,1]`.
- The **y-axis basis vector** `[0,1]` is mapped to `[1,3]`.

Visually, the transformation looks like:

![vec](/whymatmul/matrix1.png)

An important point here: the vectors themselves aren't actually moving. The original coordinate grid gets stretched and rotated so that the new “x-axis” now points toward `[2,1]`, and the new “y-axis” points toward `[1,3]`. The actual _coordinate space_ that the vectors are in is stretching and rotating which causes the vectors to stretch and rotate as well.

What if we wanted to multiply this matrix by an input vector? Starting with our matrix and input vector:

$$
M = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}
$$

We can take each row of the input vector and multiply it by each column of the matrix. Here's how it goes:

$$
\begin{bmatrix}
2 & 1 \\ 1 & 3
\end{bmatrix}
\cdot
\begin{bmatrix}
2 \\
3
\end{bmatrix}
=

\begin{bmatrix}
2\cdot2 + 1\cdot3 \\
1\cdot2 + 3\cdot3
\end{bmatrix}
=

\begin{bmatrix}
7 \\
11
\end{bmatrix}
$$

Or, visually with color codes showing the steps:

![vec](/whymatmul/dot.png)

Each row of the matrix that we multiply with the input vector, we are performing the **Dot Product** of the matrix and the vector. So a matrix–vector multiplication is just two dot products, one per row of the matrix, producing a new vector.

Let's look at this visually:

![vec](/whymatmul/mm.png)

There are a few vectors here so let's go through them:

1. The solid blue vector is the input vector of $[2,3]$. We can graph this in our coordinate plane starting from the origin $[0,0]$ and extending to $[2,3]$.
2. The solid green vector is the 1st column of the matrix $M = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, which is $\begin{bmatrix} 2 \\ 1 \end{bmatrix}$
3. The dashed green vector is the 1st element of the input vector multiplied by the 1st column of the matrix, $2 \cdot [2,1] = [4,2]$
4. The solid orange vector is the 2st column of the matrix $M = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, which is $\begin{bmatrix} 1 \\ 3 \end{bmatrix}$.
5. The dashed orange vector is the 2st element of the input vector multiplied by the 2st column of the matrix, $3 \cdot [1,3] = [3,9]$
6. The solid red vector is resulting vector when we add the dashed orange vector and the dashed green vector.

But wait! Why do we just have one result vector? I thought the matrix was a combination of vectors, so shouldn't we have two vectors? This is an excellent question and gets at the heart of matrix multiplication.

When you multiply a **matrix** by a **vector**, you get a **vector**. Why?

You get a vector because the matrix acts on the vector. That means each column of the matrix gets scaled by the corresponding element of the vector and then all of those scaled columns are added up to produce the weighted sum.

Think of it this way:

- The matrix $M$ has two "basis directions" (its columns)
- The matrix says to the input vector $[2, 3]$ "give me **2 units** of the first direction and **3 units** of the second direction"
- You add those scaled directions together to get the result vector

By taking the dot product of each matrix and vector, we're stretching and rotating the input vector by the matrix in order to project it into a new space.

The cool thing is that this scales and works with matrices that have as many dimensions as you want! Let's look at an example in the next section.

# Multiplying Matrices

We learned how to multiply a matrix by a vector above

When you perform matrix multiplication, you’re essentially doing a lot of dot products.

For example:

$$
A =
\begin{bmatrix}
a_{11} & a_{12} \
a_{21} & a_{22}
\end{bmatrix}, \quad
B =
\begin{bmatrix}
b_{11} & b_{12} \
b_{21} & b_{22}
\end{bmatrix}
$$

Then:

$$
A \times B =
\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
\end{bmatrix}
$$

Each entry in the output matrix is a **dot product** between:

- a **row** of the first matrix, and
- a **column** of the second matrix.

So, if (A) is a 2×3 matrix and (B) is a 3×2 matrix, the (i, j)th entry of (A \times B) is:

$$
(A \cdot B)_{ij} = \text{dot}(A*_{i,*}, B_{*,j})
$$

**Matrix multiplication** applies this operation across multiple pairs of vectors to project and combine data — producing a new set of vectors.

How do we think about this intuitively?

The **dot product** measures how "aligned" two vectors are. If they are very aligned then they will have a large dot product, if they are not aligned they will have a small dot product. So we can use the dot product as a measure to tell us how similar two vectors are.

Now we tie it all together.

# From Matrices to Meaning

What if these vectors don't represent directions in space, but instead represent meaning in some abstract space?

In a large language model like ChatGPT, every word is represented as a vector, typically with hundreds or thousands of dimensions instead of just 2 that we saw above. When the model sees the word "king", it's not just storing the letters k-i-n-g, it's storing a vector like `[0.23, -0.45, 0.87, ...]` that captures the meaning of "king" based on how it's used in language. It learns this meaning as it's being trained on every book, article and piece of data on the internet and at the end of that training spits out a vector like `[0.23, -0.45, 0.87, ...]`.

But how does it capture the meaning of words like "king"? I thought you'd never ask. Let's look at a simple example.

Imagine we have simple 2D "word vectors" where:

- The **first dimension** represents "royalty" (0 = common, 1 = royal)
- The **second dimension** represents "femininity" (0 = masculine, 1 = feminine)

For example:

- "king" → `[0.9, 0.1]` (high royalty, low femininity)
- "queen" → `[0.9, 0.9]` (high royalty, high femininity)
- "man" → `[0.1, 0.1]` (low royalty, low femininity)
- "woman" → `[0.1, 0.9]` (low royalty, high femininity)

Now if we do the some simple vector arithmetic: `king - man + woman`:

$$
[0.9, 0.1] - [0.0, 0.1] + [0.0, 0.9] = [0.9, 0.9]
$$

This result is `[0.9, 0.9]` which matches "queen"!

Why does this work? We:

1. Started with "king" (royal + masculine)
2. Subtracted "man" (removing the masculine aspect)
3. Added "woman" (adding the feminine aspect)
4. Ended up with a royal + feminine concept = "queen"

We've done the same weighted sum operation, except now it's operating on **semantic meaning** rather than spatial directions. We're now moving from matrix multiplication to meaning.

As we mentioned above, LLMs like ChatGPT have billions (maybe trillions!) of weights that get refined during training. Through matrix multiplications, the model discovers that words used in simil'ar contexts should have similar vectors.

So how does a neural network actually learn these meaningful representations?

# Learning Meaning

Now that we understand how matrix multiplications (also called linear transformations) stretch and rotate vectors in space, let’s connect this to how language models _learn meaning_.

When you feed a word embedding like `king` → `[0.23, -0.45, 0.87, ...]` into a neural network, the model applies a matrix multiplication to it — that’s just a linear transformation. Each column of the weight matrix represents a new **feature direction** the model has learned: maybe one captures "royalty," another "gender," another "formality," etc.

Formally, if we denote the input embedding as a vector **x** and the learned weight matrix as **W**, the layer computes:

$$
h = x \cdot W
$$

This is just a rotation and scaling of the original embedding into a new space.
But the key insight is: during training, the model learns the **geometry** of meaning — it adjusts **W** so that similar meanings land close together, and dissimilar meanings are pushed apart.

Over billions of examples, the model discovers that “dog” and “cat” should point in similar directions, while “dog” and “keyboard” should be nearly orthogonal.

---

# The Attention Mechanism: How Context Emerges

So far, we’ve looked at how a single vector (a single word) is transformed.
But ChatGPT doesn’t just understand words — it understands **sentences**, **context**, and **relationships** between words. That’s where **attention** comes in.

At its core, attention asks a simple question:

> “Given this word, which other words in the sentence should I pay attention to?”

Let’s see how that’s done mathematically — and yes, again, it’s all just matrix multiplications.

---

## Step 1: Projecting into Query, Key, and Value Spaces

For each token in the input (say, the words “The cat sat”), the model has an embedding vector.
Each of these embeddings is multiplied by **three** learned matrices:

$$
Q = X \cdot W_Q \
K = X \cdot W_K \
V = X \cdot W_V
$$

where:

- **Q (Query):** represents what this word is looking for
- **K (Key):** represents what this word contains
- **V (Value):** represents the actual information to be passed along

So each word embedding gets projected into three separate subspaces — the _asking_, _offering_, and _informing_ spaces.

---

### Diagram: Attention as MatMul Flow

```
 Input Embeddings (X)
       │
       ├── matmul → W_Q → Queries (Q)
       ├── matmul → W_K → Keys (K)
       └── matmul → W_V → Values (V)
              ↓
      Attention = softmax(Q × Kᵀ / √d)
              ↓
     Weighted sum → Attention × V
              ↓
   Contextualized Output Vectors
```

Each line above represents a matrix multiplication.

---

## Step 2: Computing “Who Looks at Whom”

Now that every token has its own Q, K, and V vectors, the model computes how strongly each token should attend to every other token using the **dot product** of Q and K:

$$
\text{attention scores} = Q \cdot K^T
$$

This is a matrix of pairwise similarities — every entry says “how much token _i_ cares about token _j_.”
For example, in the sentence:

> “The cat sat on the mat.”

the token “cat” will have a high similarity score with “sat” and “mat”, but low with “the”.

We then scale and normalize these scores:

$$
A = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right)
$$

The **softmax** ensures that all attention weights sum to 1, turning raw similarities into probabilities.

---

## Step 3: Weighted Sum of Values

Finally, each word’s **contextualized meaning** is computed as a weighted sum of the value vectors (V), using the attention scores (A):

$$
Z = A \cdot V
$$

That’s another matrix multiplication.

Now, every word embedding becomes a new vector — one that reflects its context.

For instance:

- “bank” in “river bank” and “bank” in “central bank” start the same.
- After attention, their meanings diverge based on nearby words.

---

### Diagram: Visualizing Attention Flow

```
       ┌────────────────────────────┐
       │   Q × Kᵀ  → Attention Map  │  ← (How much each word attends to others)
       └────────────────────────────┘
                    │
                    ▼
          Weighted Sum with V
                    │
                    ▼
         Contextualized Representation
```

At every layer, this process repeats — each token updates its meaning based on everything it sees around it.

---

# Why Matrix Multiplication Works (and Nothing Else Would)

So why do we use matrix multiplication — and not some other operation?

Because **matrix multiplication is the most expressive linear operation** that still allows:

1. **Composability** – multiple layers can be stacked to approximate complex nonlinear relationships.
2. **Differentiability** – gradients flow easily through matmuls during training.
3. **Geometry** – matmuls preserve spatial relationships; they rotate and scale meaning vectors in continuous space.

If we used something nonlinear or discrete (like lookup tables or symbolic rules), gradients wouldn’t propagate smoothly — and the model couldn’t learn through backpropagation.

Every matmul encodes a local, differentiable geometric transformation that contributes to global understanding.

---

# From MatMul to Coherence

Now let’s zoom out.
In one transformer block, the computation looks like this:

```
Input Embedding
    ↓
LayerNorm
    ↓
MatMul → W_Q, W_K, W_V
    ↓
Attention (Q × Kᵀ, softmax, × V)
    ↓
MatMul → W_O  (output projection)
    ↓
Residual Add
    ↓
Feed-Forward (two more matmuls + GELU)
    ↓
Residual Add
```

So even a single transformer block performs **six matrix multiplications**, each one bending the geometry of meaning slightly.

Stack hundreds of these layers — and each time, the representation of the sentence gets more contextual, abstract, and coherent.

By the time you reach the final layer, the model has rotated your prompt vector into a position that best predicts the next word.

---

### Diagram: The Lifecycle of a Token Through the Model

```
 Word Embedding → Linear Transformations → Attention → Feed-Forward → Next Token Prediction
```

Each arrow is powered by `matmul`.
Each `matmul` moves the token embedding through a high-dimensional semantic landscape — from raw text → contextual meaning → coherent continuation.

---

# Closing the Loop

When you ask ChatGPT a question, you’re really projecting your words into a vast high-dimensional vector space.
Each transformer layer — powered entirely by matrix multiplication — refines, contextualizes, and transforms these embeddings until the geometry of your input aligns with the geometry of likely next words.

So when ChatGPT replies coherently, it’s because all those `matmuls` have learned how to **rotate meaning itself**.

---

> “Every `matmul` is a thought — a rotation of meaning in a space we can’t see, but can feel.”

---

Would you like me to generate matching diagrams (in your “hand-drawn math sketch” style) for:

- the **Q/K/V attention flow**
- the **Q×Kᵀ attention map + softmax + V weighted sum**
- the **transformer block overview** (showing all the matmuls stacked)?

They’ll visually tie this narrative together perfectly.
