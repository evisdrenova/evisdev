---
title: "From Attention to Meaning"
date: "2025-10-17"
publish: false
---

import Note from "../components/Note";

This is the third installment in my "From ... to Meaning" series. So far, we've covered how [matrix multiplication](/why_matmul) transforms vectors and how and [activation functions](/why_activations) introduce nonlinearity. Now we'll explore **attention**, the mechanism that gives LLMs their ability to understand context and relationships between words.

Attention is interesting because, in my opinion, it's the easiest mechanism to grasp conceptually but the hardest to grasp technically. But I think it's critical to understand it in order to truly grok how LLMs create meaningful responses.

Like always, let's start at the beginning and build up to the intuition.

## The Context Problem

When we tokenize and embed a word, we create a single vector representation of that word. For example, the word "bank" might get transformed into `[0.23, -0.45, 0.87, ...]`. This is a fixed vector that will never change.

But here's the problem: the word "bank" means completely different things in different contexts:

For example:

- "I sat by the river bank" (shore)
- "I deposited money at the bank" (financial institution)
- "The plane started to bank left" (to tilt)

How can one fixed vector `[0.23, -0.45, 0.87, ...]` capture all three meanings? Simple, it can't.

Here's another example:

> "The animal didn't cross the street because **it** was too tired."

What does "it" refer to? The animal.

> "The animal didn't cross the street because **it** was too wide."

Now "it" refers to the street. Same word, completely different meaning based on context.

This is the fundamental challenge that attention solves: creating dynamic, context-dependent representations through matrix multiplication.

## The Core Idea

At its core, attention asks a simple question:

> “Given this word, which other words in the sentence should I pay attention to?”

When you read "The cat sat on the mat because it was soft," your brain automatically:

1. **Searches** through previous words (cat? mat?)
2. **Scores** which ones are relevant (mat is more relevant for "soft")
3. **Combines** those relevant words to understand "it" means "mat"

You brain does this because it's been trained over many years of reading and speaking language. Likewise, an LLM must also be trained on a massive corpus of data in order to learn this context. As it's trained, it learns three weight matrices that give it the contextual mechanism to decipher language and it's context.

## The Three Projections: Query, Key, Value

Attention solves the context problem by letting each word in the sentence or context look at every other word in the sentence or context and decide what to pay attention to and how much to pay attention to it.

For every word in the sentence, we create three different "views" of that word by multiplying its embedding ($X$) by three different learned weight matrices:

![ve](/whyattention/em.png)

Here's the intuition:

- **Query (Q)**: Represents what this word is _searching for_ in other words. It encodes what kind of information this token wants to retrieve from others. For example, in the phrase “the cat sat on the mat”, the word “sat” might query for subjects (who sat?), so its query vector looks for noun-like patterns.

- **Key (K)**: Represents what this word _offers_ to other words. The key is like a descriptor tag that advertises what kind of information a token carries. In the same example as above, “cat”’s key might encode “noun / animal / subject,” so when “sat”’s query compares itself to “cat”’s key (via a dot product), they match strongly.

- **Value (V)**: Represents the actual _information_ this word will contribute when selected. You can think of it as the payload or content that gets passed forward after the attention weights are applied.

Another way to think about it is using a database analogy.

- You type **Query** terms into a search bar ("machine learning papers")
- The database checks these against **Key** indices/tags on each document
- When there's a match, you retrieve the **Value**, the actual document content

In attention, every word is simultaneously:

- Searching for relevant information in every other word (via its Query)
- Advertising what it contains to every other word (via its Key)
- Ready to share its information to every other word (via its Value)

## Step-by-Step: How Attention Works

Let's walk through a concrete example with the sentence: "cat sat mat"

Each word starts with an embedding vector (simplified to 4D):

```
cat = [1.0, 0.2, 0.5, 0.1]
sat = [0.3, 1.0, 0.2, 0.4]
mat = [0.8, 0.1, 0.9, 0.2]
```

### Step 1: Create Q, K, V for Each Word

We multiply each embedding by the three weight matrices:

$$Q_{\text{cat}} = \text{cat} \cdot W_Q$$
$$K_{\text{cat}} = \text{cat} \cdot W_K$$
$$V_{\text{cat}} = \text{cat} \cdot W_V$$

(Same for "sat" and "mat")

### Step 2: Calculate Attention Scores

For each word, we compute: **"How much should I pay attention to every other word?"**

Let's focus on "mat". Its query $Q_{\text{mat}}$ is compared against all keys using dot products:

$$\text{score}_{\text{mat},\text{cat}} = Q_{\text{mat}} \cdot K_{\text{cat}}^T$$
$$\text{score}_{\text{mat},\text{sat}} = Q_{\text{mat}} \cdot K_{\text{sat}}^T$$
$$\text{score}_{\text{mat},\text{mat}} = Q_{\text{mat}} \cdot K_{\text{mat}}^T$$

High dot product = high similarity = high relevance.

Then we apply **softmax** to turn scores into probabilities:

$$\text{attention}_{\text{mat}} = \text{softmax}([\text{scores}])$$

Result might be: `[0.1, 0.2, 0.7]` meaning:

- 10% attention to "cat"
- 20% attention to "sat"
- 70% attention to "mat" (itself)

### Step 3: Weighted Sum of Values

Now we create a new representation as a weighted combination:

$$\text{output}_{\text{mat}} = 0.1 \cdot V_{\text{cat}} + 0.2 \cdot V_{\text{sat}} + 0.7 \cdot V_{\text{mat}}$$

**This is the same weighted sum concept from our matrix multiplication blog!** We're taking a linear combination of value vectors, weighted by relevance.

### Diagram 2: Attention Flow for One Word

```
Query (mat)
    |
    | dot product with all Keys
    ↓
[score_cat, score_sat, score_mat]
    |
    | softmax
    ↓
[0.1, 0.2, 0.7]  ← attention weights
    |
    | weighted sum of Values
    ↓
New representation of "mat"
```

_Simple diagram: Show "mat" at top, three boxes for scores, then softmax arrow, then weighted combination of three "V" boxes_

---

## The Full Attention Equation

For all words simultaneously, attention is computed as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Let's break down what's happening:

1. **$QK^T$**: Matrix multiply all queries against all keys → attention score matrix  
   (every entry is "how much token i cares about token j")

2. **$\frac{1}{\sqrt{d_k}}$**: Scale by dimension to prevent extreme values

3. **softmax**: Convert scores to probabilities (each row sums to 1)

4. **multiply by V**: Weight the values by attention scores → final output

### Diagram 3: Attention Matrix Visualization

```
     cat   sat   mat
cat [0.5   0.3   0.2]    ← attention weights for "cat"
sat [0.2   0.6   0.2]    ← attention weights for "sat"
mat [0.1   0.2   0.7]    ← attention weights for "mat"

Each row: how much this word attends to all others
```

_Simple diagram: 3x3 grid with darker shading on diagonal, showing attention matrix_

---

## Why This Solves the Context Problem

Remember our "it" problem?

> "The animal didn't cross the street because **it** was too tired."

When attention processes "it":

1. Its **query** asks: "What noun am I referring to?"
2. Compares against **keys** from "animal" and "street"
3. "animal" + "tired" have high similarity → high attention score
4. Pulls in the **value** vector from "animal"
5. **Now "it" has a representation similar to "animal"!**

For the other sentence:

> "The animal didn't cross the street because **it** was too wide."

Same process, but:

- "street" + "wide" have high similarity → high attention score
- "it" pulls in the **value** vector from "street"
- **Now "it" has a representation similar to "street"!**

**The same word gets different representations based on context, all through learned matrix multiplications.**

### Diagram 4: Context-Dependent "it"

```
Sentence 1: "animal ... it ... tired"
                ↓
            it → [high attention to "animal"]
                ↓
            it ≈ animal vector

Sentence 2: "street ... it ... wide"
                ↓
            it → [high attention to "street"]
                ↓
            it ≈ street vector
```

_Simple diagram: Two parallel flows showing "it" getting different representations_

---

## From One Layer to 96 Layers

ChatGPT doesn't use just one attention layer — it uses **96 layers** stacked on top of each other. Each layer refines representations in increasingly abstract ways:

**Layers 1-10: Syntax and Grammar**

- Which words are nouns, verbs, adjectives?
- Phrase boundaries and word relationships

**Layers 11-30: Semantic Relationships**

- Entity recognition (Barack Obama is a president)
- Relationships (Paris is capital of France)
- Coreference resolution

**Layers 31-60: Reasoning and Inference**

- Logical deduction (if A→B and B→C, then A→C)
- Analogical reasoning (king:queen :: man:woman)

**Layers 61-90: Task Understanding**

- What kind of question is this?
- What format should the answer take?

**Layers 91-96: Output Generation**

- Selecting specific words to generate
- Maintaining coherence

### Diagram 5: Layer Hierarchy

```
Input: "bank" (in financial context)

Layer 1:  [raw embedding]
Layer 20: [+ nearby word context]
Layer 50: [+ sentence structure]
Layer 80: [+ discourse meaning]
Layer 96: [ready for prediction]

Each layer adds more abstract understanding
```

_Simple diagram: Vertical stack of boxes, each slightly wider, showing progressive refinement_

---

## A Complete Transformer Block

Each layer performs multiple matrix multiplications:

```
Input Embedding
    ↓
LayerNorm
    ↓
[Attention Block]
    Q = X · W_Q
    K = X · W_K
    V = X · W_V
    Attention = softmax(QK^T/√d) · V
    Output = Attention · W_O
    ↓
Residual Add (+ Input)
    ↓
LayerNorm
    ↓
[Feed-Forward Block]
    Hidden = Output · W_1
    Hidden = GELU(Hidden)
    Final = Hidden · W_2
    ↓
Residual Add
    ↓
Next Layer
```

**Count the matmuls**: 4 in attention (Q, K, V, output) + 2 in feed-forward = **6 matmuls per layer**

For 96 layers: **576 matrix multiplications per forward pass**

### Diagram 6: One Transformer Block

```
     Input
       |
   [LayerNorm]
       |
   [Attention] ←-- 4 matmuls
       |
      (+) ←------- residual
       |
   [LayerNorm]
       |
[Feed-Forward] ←-- 2 matmuls
       |
      (+) ←------- residual
       |
     Output
```

_Simple diagram: Vertical flow with two residual connections shown as curved arrows_

---

## Following a Thought Through ChatGPT

Let's trace what happens when you ask: **"Why is the sky blue?"**

### Token → Embedding (Matmul #1)

Tokens: `["Why", "is", "the", "sky", "blue", "?"]`

Each token ID is multiplied by embedding matrix:
$$\text{Why}_0 = \text{TokenID}_{324} \cdot W_{\text{embed}}$$

### Layer 1: Basic Syntax (Matmuls #2-7)

After attention:

- "Why" recognized as question word
- "is" identified as linking verb
- "sky" and "blue" marked as key content words

### Layer 20: Semantic Understanding (Matmuls #114-119)

After many refinements:

- "sky" understood as atmosphere
- "blue" understood as color perception
- "Why" indicates causal explanation needed
- Activates concepts: light, wavelengths, scattering

### Layer 50: Knowledge Retrieval (Matmuls #294-299)

Through matrix multiplications, the model finds:
$$\text{sky} + \text{blue} + \text{why} \rightarrow \text{[high activation in "light scattering" region]}$$

The knowledge isn't looked up — **it IS the weights themselves**.

### Layer 96: Word Selection (Matmuls #570-576)

Final projection to vocabulary:
$$\text{logits} = \text{hidden}_{96} \cdot W_{\text{output}}$$

Produces probability distribution:

```
P("The") = 0.73
P("Sky") = 0.15
P("Blue") = 0.08
...
```

Picks "The", adds it to context, repeats for next word.

### Diagram 7: Information Flow Through Layers

```
"Why is the sky blue?"
        ↓
  [Embeddings]
        ↓
    [Layer 1]  ← syntax
        ↓
   [Layer 20]  ← semantics
        ↓
   [Layer 50]  ← reasoning
        ↓
   [Layer 96]  ← generation
        ↓
"The sky is blue because..."
```

_Simple diagram: Vertical flow with labels on right showing what each layer does_

---

## Why Matrix Multiplication Works

Matrix multiplication is the foundation because it's the most expressive linear operation that allows:

1. **Composability**: Multiple layers can be stacked to approximate complex nonlinear relationships

2. **Differentiability**: Gradients flow easily through matmuls during training, enabling backpropagation

3. **Geometric intuition**: Matmuls rotate and scale vectors in continuous space, preserving spatial relationships

4. **Expressiveness**: Can represent any linear transformation, including weighted combinations

If we used discrete operations (lookup tables, symbolic rules), gradients wouldn't propagate — the model couldn't learn through backpropagation.

---

## The Final Insight: Why Attention Creates Meaning

Attention works because **it's the mathematical operation that says "this depends on a weighted combination of those."**

And that's exactly what language understanding requires:

- **Meaning emerges from relationships**: A word's meaning comes from its relationships to other words. Matrix multiplication encodes these relationships.

- **Context determines everything**: Same word, different contexts → attention dynamically reweights representations based on surrounding words.

- **Understanding requires synthesis**: To answer "why is the sky blue?" requires combining knowledge about light, atmosphere, physics, and question-answering. Attention creates these combinations.

- **Coherence comes from consistency**: By using the same operation (matmul) throughout, the model learns consistent representations across all linguistic levels.

When you type "Why is the sky blue?", you trigger a cascade of **trillions of matrix multiplications**:

- Through 96 layers of abstraction
- With 175 billion learned weights
- Producing weighted combinations optimized over billions of training examples

Each attention operation asks: "Given this word, which other words matter?" And through learned Q, K, V matrices, it finds the answer — creating context-aware representations that capture the full richness of language.

**Attention is what makes LLMs feel "human"** — because it gives them the ability to understand that meaning doesn't live in individual words, but in the relationships between them.

---

> "Every attention operation is a question: 'What should I focus on?' And the answer, learned from billions of examples, is what transforms static symbols into dynamic understanding."

---

## Summary: The Attention Mechanism

1. **Project** each word into Q, K, V spaces (3 matmuls)
2. **Compare** queries against keys to get attention scores (1 matmul: $QK^T$)
3. **Normalize** scores with softmax (probabilities sum to 1)
4. **Combine** values using attention weights (1 matmul: $\text{scores} \cdot V$)
5. **Stack** this 96 times for hierarchical understanding

At every layer, words update their meaning based on context. By the final layer, the model has created representations that encode not just what words are, but what they _mean_ in this specific context — ready to generate coherent, contextually-appropriate responses.

**It's matrix multiplication all the way down.**
