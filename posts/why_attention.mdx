---
title: "From Attention to Meaning"
date: "2025-10-17"
publish: false
---

import Note from "../components/Note";

This is the third installment in my "From ... to Meaning" series. So far, we've covered how [matrix multiplication](/why_matmul) transforms vectors and how and [activation functions](/why_activations) introduce nonlinearity. Now we'll explore **attention**, the mechanism that gives LLMs their ability to understand context and relationships between words.

Attention is interesting because, in my opinion, it's the easiest mechanism to grasp conceptually but the hardest to grasp technically. But I think it's critical to understand it in order to truly grok how LLMs create meaningful responses.

Like always, let's start at the beginning and build up to the intuition.

# The Context Problem

When we tokenize and embed a word, we create a single vector representation of that word. For example, the word "bank" might get transformed into `[0.23, -0.45, 0.87, ...]`. This is a fixed vector that will never change.

But here's the problem: the word "bank" means completely different things in different contexts:

For example:

- "I sat by the river bank" (shore)
- "I deposited money at the bank" (financial institution)
- "The plane started to bank left" (to tilt)

How can one fixed vector `[0.23, -0.45, 0.87, ...]` capture all three meanings? Simple, it can't.

Here's another example:

> "The animal didn't cross the street because **it** was too tired."

What does "it" refer to? The animal.

> "The animal didn't cross the street because **it** was too wide."

Now "it" refers to the street. Same word, completely different meaning based on context.

This is the fundamental challenge that attention solves: creating dynamic, context-dependent representations through matrix multiplication.

# The Core Idea

At its core, attention asks a simple question:

> “Given this word, which other words in the sentence should I pay attention to?”

When you read "The cat sat on the mat because it was soft," your brain automatically:

1. **Searches** through previous words (cat? mat?)
2. **Scores** which ones are relevant (mat is more relevant for "soft")
3. **Combines** those relevant words to understand "it" means "mat"

You brain does this because it's been trained over many years of reading and speaking language. Likewise, an LLM must also be trained on a massive corpus of data in order to learn this context. As it's trained, it learns three weight matrices that give it the contextual mechanism to decipher language and it's context.

# Three Projections: Query, Key, Value

Attention solves the context problem by letting each word in the sentence "look" at every other word in the sentence and decide what to pay attention to and how much to pay attention to it.

For every word in the sentence, we create three different "views" of that word by multiplying its embedding ($X$) by three different learned weight matrices called the **Query**, **Key** and **Value** matrices.

Using the word "bank" from the example above, we would multiply its embedding by each of the weight matrices like this:

![ve](/whyattention/em.png)

But what do each of these weight matrices represent? Here's the intuition:

- **Query (Q)**: Represents what this word is _searching for_ in other words. It encodes what kind of information this token wants to retrieve from others. For example, in the phrase “the cat sat on the mat”, the word “sat” might query for subjects (who sat?), so its query vector looks for noun-like patterns.

- **Key (K)**: Represents what this word _offers_ to other words. The key is like a descriptor tag that advertises what kind of information a token carries. In the same example as above, “cat”’s key might encode “noun / animal / subject,” so when “sat”’s query compares itself to “cat”’s key (via a dot product), they match strongly.

- **Value (V)**: Represents the actual _information_ this word will contribute when selected. You can think of it as the payload or content that gets passed forward after the attention weights are applied.

Another way to think about it is using a database analogy.

- You type **Query** terms into a search bar ("machine learning papers")
- The database checks these against **Key** indices/tags on each document
- When there's a match, you retrieve the **Value**, the actual document content

In attention, every word is simultaneously:

- Searching for relevant information in every other word (via its Query)
- Advertising what it contains to every other word (via its Key)
- Ready to share its information to every other word (via its Value)

# Step-by-Step: How Attention Works

Let's walk through a complete example with the sentence: **"the cat sat on the mat"**.

Each word starts with an embedding vector (we'll use 4 dimensions to keep it simple):

```
the = [0.5, 0.8, 0.1, 0.3]
cat = [1.0, 0.2, 0.5, 0.1]
sat = [0.3, 1.0, 0.2, 0.4]
on  = [0.6, 0.4, 0.7, 0.5]
the = [0.5, 0.8, 0.1, 0.3]
mat = [0.8, 0.1, 0.9, 0.2]
```

## Step 1: Create Q, K, V for Each Word

We multiply each embedding by three weight matrices. Let's assume our learned weight matrices are (4×3 to project to 3D for simplicity):

$$
W_Q = \begin{bmatrix} 0.2 & 0.1 & 0.3 \\ 0.4 & 0.2 & 0.1 \\ 0.1 & 0.5 & 0.2 \\ 0.3 & 0.1 & 0.4 \end{bmatrix}, \quad
W_K = \begin{bmatrix} 0.3 & 0.2 & 0.1 \\ 0.1 & 0.4 & 0.3 \\ 0.2 & 0.1 & 0.5 \\ 0.4 & 0.3 & 0.2 \end{bmatrix}, \quad
W_V = \begin{bmatrix} 0.1 & 0.3 & 0.2 \\ 0.2 & 0.1 & 0.4 \\ 0.5 & 0.2 & 0.1 \\ 0.3 & 0.4 & 0.3 \end{bmatrix}
$$

If you remember from the "From Matmul to Meaning" blog, multiplying a vector with a matrix returns a vector. Let's do "cat" as an example:
$$Q_\text{cat} = [1.0, 0.2, 0.5, 0.1] \cdot W_Q = [0.36, 0.28, 0.39]$$
$$K_\text{cat} = [1.0, 0.2, 0.5, 0.1] \cdot W_K = [0.46, 0.33, 0.36]$$
$$V_\text{cat} = [1.0, 0.2, 0.5, 0.1] \cdot W_V = [0.42, 0.45, 0.41]$$

After computing Q, K, V for all words, we have:

```
Queries (Q):               Keys (K):                Values (V):
Q_the = [0.30, 0.35, 0.23]  K_the = [0.34, 0.41, 0.28]  V_the = [0.38, 0.43, 0.32]
Q_cat = [0.36, 0.28, 0.39]  K_cat = [0.46, 0.33, 0.36]  V_cat = [0.42, 0.45, 0.41]
Q_sat = [0.26, 0.44, 0.29]  K_sat = [0.29, 0.52, 0.38]  V_sat = [0.35, 0.38, 0.48]
Q_on  = [0.33, 0.40, 0.35]  K_on  = [0.41, 0.45, 0.42]  V_on  = [0.48, 0.41, 0.39]
Q_the = [0.30, 0.35, 0.23]  K_the = [0.34, 0.41, 0.28]  V_the = [0.38, 0.43, 0.32]
Q_mat = [0.31, 0.38, 0.44]  K_mat = [0.37, 0.34, 0.51]  V_mat = [0.51, 0.49, 0.37]
```

## Step 2: Calculate Attention Scores

Now that we have all of our Q,K,V for every word, we will calculate our attention scores. Attentions scores are calculated by taking the dot product betwee the Q and K vectors of each word.

Why do we take the dot product between Q and K vectors? Remember that the Q vector represents what kind of information this token is searching for, and the K vector represents what information this word offers. So we have a search query and potential matches — we just need to see how well they align. That's where the dot product comes into play.

The dot product measures similarity between vectors: when two vectors point in similar directions (representing similar semantic content), their dot product is large. When they point in different directions, the dot product is small. A high dot product = high similarity = high relevance = this word should pay more attention here.

Let's focus on the word "mat" (last word). Its query $Q_\text{mat} = [0.31, 0.38, 0.44]$ is compared against all keys using dot products:

$$\text{score}_\text{mat,the} = Q_\text{mat} \cdot K_\text{the}^T = 0.31 \times 0.34 + 0.38 \times 0.41 + 0.44 \times 0.28 = 0.38$$
$$\text{score}_\text{mat,cat} = Q_\text{mat} \cdot K_\text{cat}^T = 0.31 \times 0.46 + 0.38 \times 0.33 + 0.44 \times 0.36 = 0.43$$
$$\text{score}_\text{mat,sat} = Q_\text{mat} \cdot K_\text{sat}^T = 0.31 \times 0.29 + 0.38 \times 0.52 + 0.44 \times 0.38 = 0.45$$
$$\text{score}_\text{mat,on} = Q_\text{mat} \cdot K_\text{on}^T = 0.31 \times 0.41 + 0.38 \times 0.45 + 0.44 \times 0.42 = 0.49$$
$$\text{score}_\text{mat,the} = Q_\text{mat} \cdot K_\text{the}^T = 0.31 \times 0.34 + 0.38 \times 0.41 + 0.44 \times 0.28 = 0.38$$
$$\text{score}_\text{mat,mat} = Q_\text{mat} \cdot K_\text{mat}^T = 0.31 \times 0.37 + 0.38 \times 0.34 + 0.44 \times 0.51 = 0.57$$

**Raw scores**: `[0.38, 0.43, 0.45, 0.49, 0.38, 0.57]`

Notice that "mat" has the highest attention to itself (0.57) and to nearby contextual words like "on" (0.49). High dot product = high similarity = high relevance.

Also, notice that $Q_\text{mat}$ and $K_\text{mat}^T$ are different despite starting from the same base embedding. This is because each weight matrix (Q,K,V) is different so when you multiply the same embedding by different matrices you get a different output vector.

Next we apply **softmax** to turn these scores into probabilities that sum to 1:

$$\text{attention}_\text{mat} = \text{softmax}([0.38, 0.43, 0.45, 0.49, 0.38, 0.57])$$

**After softmax**: `[0.12, 0.14, 0.15, 0.16, 0.12, 0.18]`

Why do we turn these into probabilities using softmax? Why not just use the raw attention scores? This is a good question.

Softmax does two critical things:

1. **Normalization**: It forces all the attention weights to sum to 1.0 (100%). This means we're creating a proper weighted average, we can't "over-attend" by having weights that sum to more than 100%, and we ensure every word contributes proportionally.

2. **Amplifies differences**: Softmax is exponential, which means it amplifies differences between scores. If one word has a score of 0.57 and another has 0.38, softmax makes this difference even more pronounced in the final weights (0.18 vs 0.12). This helps the model make clearer decisions about what to attend to.

Without softmax, using raw scores would cause problems:

- The scores could sum to any arbitrary number making it unclear how much to attend to each word proportionally
- Small differences in relevance wouldn't be emphasized enough
- Training would be less stable because the scale of attention weights would vary wildly across different examples

Back to our example, this means "mat" will:

- Pay 12% attention to "the" (first)
- Pay 14% attention to "cat"
- Pay 15% attention to "sat"
- Pay 16% attention to "on"
- Pay 12% attention to "the" (second)
- Pay 18% attention to itself

These weights sum to approximately 1.0 (100%) giving us nice, proportional weights to work with.

## Step 3: Weighted Sum of Values

Now we create a new contextualized representation of "mat" as a weighted combination of all value vectors multipled by the probabilities that we calculated above:

$$\text{output}_\text{mat} = 0.12 \cdot V_\text{the} + 0.14 \cdot V_\text{cat} + 0.15 \cdot V_\text{sat} + 0.16 \cdot V_\text{on} + 0.12 \cdot V_\text{the} + 0.18 \cdot V_\text{mat}$$

Substituting our value vectors:
$$\text{output}_\text{mat} = 0.12 \cdot [0.38, 0.43, 0.32] + 0.14 \cdot [0.42, 0.45, 0.41] + \ldots$$

$$\text{output}_\text{mat} = [0.45, 0.44, 0.38]$$

This is the new contextualized representation of "mat" that incorporates information from the entire sentence, especially from "on" and itself, creating a representation that understands "mat" in the context of being something a cat sat on!

Next, we just repeat this same process simultaneously for all words

- "the" (first) attends mostly to "cat" (the noun it modifies)
- "cat" attends to "the" (its determiner) and "sat" (its action)
- "sat" attends to "cat" (the subject) and "mat" (the object via "on")
- "on" attends to "sat" (the verb) and "mat" (the object it relates)
- "the" (second) attends to "mat" (the noun it modifies)
- "mat" attends to "on" and itself (as we computed above)

Each word gets a new representation that captures its meaning in the context of the sentence.

<Note>

If you've ever heard of a KV cache, this is where it comes into play. When generating text token-by-token (like ChatGPT does), the model has already computed the K and V vectors for all previous tokens. Rather than recomputing them from scratch for every new token, we can cache and reuse them.

So if you've generated 100 tokens, and you're generating token 101, you don't need to recompute the Q, K, V vectors for tokens 1-100. You only compute Q, K, V for the new token, then compute attention scores between the new Q and all the cached K vectors, and finally combine the cached V vectors.

This optimization makes generation dramatically faster. Instead of doing O(n²) computations for each new token (where n is sequence length), we only do O(n) computations (because we only have to generate attention scores for just the new token instead of every token with every other token). For a 1000-token conversation, this means ~1000x speedup per token generated!

The problem is that KV caches consume a lot of memory (storing K and V for every token at every layer) which is why we have limits on context windows.

</Note>

### Diagram 2: Attention Flow for One Word

```
Query (mat)
    |
    | dot product with all Keys
    ↓
[score_the, score_cat, score_sat, score_on, score_the, score_mat]
    |
    | softmax
    ↓
[0.12, 0.14, 0.15, 0.16, 0.12, 0.18]  ← attention weights
    |
    | weighted sum of Values
    ↓
New representation of "mat"
```

_Simple diagram: Show "mat" at top, six boxes for scores, then softmax arrow, then weighted combination of six "V" boxes_

---

## The Full Attention Equation

For all words simultaneously, attention is computed as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Let's break down what's happening:

1. **$QK^T$**: Matrix multiply all queries against all keys → attention score matrix  
   (every entry is "how much token i cares about token j")

2. **$\frac{1}{\sqrt{d_k}}$**: Scale by dimension to prevent extreme values

3. **softmax**: Convert scores to probabilities (each row sums to 1)

4. **multiply by V**: Weight the values by attention scores → final output

### Diagram 3: Attention Matrix Visualization

```
     cat   sat   mat
cat [0.5   0.3   0.2]    ← attention weights for "cat"
sat [0.2   0.6   0.2]    ← attention weights for "sat"
mat [0.1   0.2   0.7]    ← attention weights for "mat"

Each row: how much this word attends to all others
```

_Simple diagram: 3x3 grid with darker shading on diagonal, showing attention matrix_

---

## Why This Solves the Context Problem

Remember our "it" problem?

> "The animal didn't cross the street because **it** was too tired."

When attention processes "it":

1. Its **query** asks: "What noun am I referring to?"
2. Compares against **keys** from "animal" and "street"
3. "animal" + "tired" have high similarity → high attention score
4. Pulls in the **value** vector from "animal"
5. **Now "it" has a representation similar to "animal"!**

For the other sentence:

> "The animal didn't cross the street because **it** was too wide."

Same process, but:

- "street" + "wide" have high similarity → high attention score
- "it" pulls in the **value** vector from "street"
- **Now "it" has a representation similar to "street"!**

**The same word gets different representations based on context, all through learned matrix multiplications.**

### Diagram 4: Context-Dependent "it"

```
Sentence 1: "animal ... it ... tired"
                ↓
            it → [high attention to "animal"]
                ↓
            it ≈ animal vector

Sentence 2: "street ... it ... wide"
                ↓
            it → [high attention to "street"]
                ↓
            it ≈ street vector
```

_Simple diagram: Two parallel flows showing "it" getting different representations_

---

## From One Layer to 96 Layers

ChatGPT doesn't use just one attention layer — it uses **96 layers** stacked on top of each other. Each layer refines representations in increasingly abstract ways:

**Layers 1-10: Syntax and Grammar**

- Which words are nouns, verbs, adjectives?
- Phrase boundaries and word relationships

**Layers 11-30: Semantic Relationships**

- Entity recognition (Barack Obama is a president)
- Relationships (Paris is capital of France)
- Coreference resolution

**Layers 31-60: Reasoning and Inference**

- Logical deduction (if A→B and B→C, then A→C)
- Analogical reasoning (king:queen :: man:woman)

**Layers 61-90: Task Understanding**

- What kind of question is this?
- What format should the answer take?

**Layers 91-96: Output Generation**

- Selecting specific words to generate
- Maintaining coherence

### Diagram 5: Layer Hierarchy

```
Input: "bank" (in financial context)

Layer 1:  [raw embedding]
Layer 20: [+ nearby word context]
Layer 50: [+ sentence structure]
Layer 80: [+ discourse meaning]
Layer 96: [ready for prediction]

Each layer adds more abstract understanding
```

_Simple diagram: Vertical stack of boxes, each slightly wider, showing progressive refinement_

---

## A Complete Transformer Block

Each layer performs multiple matrix multiplications:

```
Input Embedding
    ↓
LayerNorm
    ↓
[Attention Block]
    Q = X · W_Q
    K = X · W_K
    V = X · W_V
    Attention = softmax(QK^T/√d) · V
    Output = Attention · W_O
    ↓
Residual Add (+ Input)
    ↓
LayerNorm
    ↓
[Feed-Forward Block]
    Hidden = Output · W_1
    Hidden = GELU(Hidden)
    Final = Hidden · W_2
    ↓
Residual Add
    ↓
Next Layer
```

**Count the matmuls**: 4 in attention (Q, K, V, output) + 2 in feed-forward = **6 matmuls per layer**

For 96 layers: **576 matrix multiplications per forward pass**

### Diagram 6: One Transformer Block

```
     Input
       |
   [LayerNorm]
       |
   [Attention] ←-- 4 matmuls
       |
      (+) ←------- residual
       |
   [LayerNorm]
       |
[Feed-Forward] ←-- 2 matmuls
       |
      (+) ←------- residual
       |
     Output
```

_Simple diagram: Vertical flow with two residual connections shown as curved arrows_

---

## Following a Thought Through ChatGPT

Let's trace what happens when you ask: **"Why is the sky blue?"**

### Token → Embedding (Matmul #1)

Tokens: `["Why", "is", "the", "sky", "blue", "?"]`

Each token ID is multiplied by embedding matrix:
$$\text{Why}_0 = \text{TokenID}_{324} \cdot W_{\text{embed}}$$

### Layer 1: Basic Syntax (Matmuls #2-7)

After attention:

- "Why" recognized as question word
- "is" identified as linking verb
- "sky" and "blue" marked as key content words

### Layer 20: Semantic Understanding (Matmuls #114-119)

After many refinements:

- "sky" understood as atmosphere
- "blue" understood as color perception
- "Why" indicates causal explanation needed
- Activates concepts: light, wavelengths, scattering

### Layer 50: Knowledge Retrieval (Matmuls #294-299)

Through matrix multiplications, the model finds:
$$\text{sky} + \text{blue} + \text{why} \rightarrow \text{[high activation in "light scattering" region]}$$

The knowledge isn't looked up — **it IS the weights themselves**.

### Layer 96: Word Selection (Matmuls #570-576)

Final projection to vocabulary:
$$\text{logits} = \text{hidden}_{96} \cdot W_{\text{output}}$$

Produces probability distribution:

```
P("The") = 0.73
P("Sky") = 0.15
P("Blue") = 0.08
...
```

Picks "The", adds it to context, repeats for next word.

### Diagram 7: Information Flow Through Layers

```
"Why is the sky blue?"
        ↓
  [Embeddings]
        ↓
    [Layer 1]  ← syntax
        ↓
   [Layer 20]  ← semantics
        ↓
   [Layer 50]  ← reasoning
        ↓
   [Layer 96]  ← generation
        ↓
"The sky is blue because..."
```

_Simple diagram: Vertical flow with labels on right showing what each layer does_

# Wrapping up

When you type "Why is the sky blue?" into ChatGPT, you trigger a cascade of **trillions of matrix multiplications**:

- Through 96 layers of abstraction
- With hundreds of billions of learned weights
- Producing weighted combinations optimized over billions of training examples

Each attention operation asks: "Given this word, which other words matter?" And through learned Q, K, V matrices, it finds the answer, creating context-aware representations that capture the full scale of language.

And the cool thing is that it's just it's matrix multiplication all the way down.
