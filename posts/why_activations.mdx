---
title: "From Activations to Meaning"
date: "2025-10-07"
publish: false
---

import Note from "../components/Note";

In my last post, [From Matmul to Meaning](/why_matmul), I explained how matrix multiplication is the fundamental operation that lets LLMs understand meaning. We saw how billions of matmuls transform word vectors through semantic space until they produce coherent responses.

At the end of that post, I mentioned activation functions as a critical piece I didn't cover (mainly because the blog was already getting long) but without activation functions, matrix multiplication can't learn anything interesting.

Even a 96-layer LLM like GPT-4 would collapse into a single matrix multiplication. How is this possible? How can 96 layers of transformations collapse into one?

Let's start at the beginning and build our way to the intuition.

# The Problem With Matmul

Let's say we have a simple 2-layer neural network.

$$
h_1 = x \cdot W_1
$$

$$
\hat{y} = h_1 \cdot W_2
$$

Where $x$ is our input (our vector), $W_1$ and $W_2$ are weight matrices, and $h_1$ is the hidden layer and $\hat{y}$ is the output layer.

Visually, this looks like:

![ve](/whyactivation/nn.png)

<Note>

The weights in this diagram are the connections (lines) between nodes. The lines from the input to the hidden layer represent the W₁ matrix, and the lines from hidden to output represent the W₂ matrix. Each line is one weight value, and together they form the complete weight matrices.

We can represent this matrix like this:

$$
W_1 = \begin{bmatrix}
w_{x_1 h_1} & w_{x_1 h_2} & w_{x_1 h_3} \\
w_{x_2 h_1} & w_{x_2 h_2} & w_{x_2 h_3} \\
w_{x_3 h_1} & w_{x_3 h_2} & w_{x_3 h_3}
\end{bmatrix}, \quad
W_2 = \begin{bmatrix}
w_{h_1 \hat{y}} \\
w_{h_2 \hat{y}} \\
w_{h_3 \hat{y}}
\end{bmatrix}
$$

Where the $w_{x_1 h_1}$ represents the weight going from the ${x_1}$ node to the ${h_1}$ node.

</Note>

We can simplify our matrices above by substituting the first equation into the second (reading from left to right):

$$
\hat{y} = (x \cdot W_1) \cdot W_2 = x \cdot (W_1 \cdot W_2)
$$

We know that we can easily multiply two matrices together and get a single resulting matrix, so we can combine $W_1 \cdot W_2$ into a single matrix $W$:

$$
\hat{y} = x \cdot W
$$

Our "2-layer" network just collapsed into a single matrix multiplication!

Let's do this with actual numbers to really drive the point home. Suppose:

$$
x = [1, 2], \quad W_1 = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad W_2 = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}
$$

**Layer 1:**

$$
h_1 = [1, 2] \cdot \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} = [4, 7]
$$

**Layer 2:**

$$
\hat{y} = [4, 7] \cdot \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} = [18, 15]
$$

Now let's compute $W_1 \cdot W_2$ directly:

$$
W = W_1 \cdot W_2 = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \cdot \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 5 \\ 7 & 5 \end{bmatrix}
$$

**Single layer:**

$$
\hat{y} = [1, 2] \cdot \begin{bmatrix} 4 & 5 \\ 7 & 5 \end{bmatrix} = [18, 15]
$$

Same answer!

This is called **linearity**, and it holds true for any number of layers in a neural network that are purely doing matrix multiplication. No matter how many layers you stack, pure matrix multiplication can only represent linear transformations: rotations, scalings, and shears.

Okay, so layer collapse is bad. But why? But why aren't linear transformations enough?

# Why Matmul Isn't Enough

Remember our "king - man + woman = queen" example from the previous blog post? If not, no worries. Let's say we have a simple 2D "word space" where:

- The first dimension represents _royalty_ (0 = common, 1 = royal)
- The second dimension represents _femininity_ (0 = masculine, 1 = feminine)

So our mini vocabulary looks like this:

| Word      | Vector (royalty, femininity) |
| --------- | ---------------------------- |
| **king**  | [0.9, 0.1]                   |
| **queen** | [0.9, 0.9]                   |
| **man**   | [0.1, 0.1]                   |
| **woman** | [0.1, 0.9]                   |

To get from "king" to "queen", we can do some simple vector math:

$$
\text{king} - \text{man} + \text{woman} = [0.9, 0.1] - [0.1, 0.1] + [0.1, 0.9] = [0.9, 0.9] = \text{queen}
$$

This works because it was a _linear_ operation in vector space. This is linear because:

- We know: king → `[0.9, 0.1]`, man → `[0.1, 0.1]`, woman → `[0.1, 0.9]`
- We can predict: king - man + woman → `[0.9, 0.1]` - `[0.1, 0.1]` + `[0.1, 0.9]` = `[0.9, 0.9]`

The result is exactly the sum of the individual transformations. That's what makes it linear.

But language is complex and rarely linear.

- "Bank" means different things near "river" vs "money"
- "Not bad" means "good" (negation reverses sentiment)
- "The animal didn't cross because it was tired" requires reasoning

Linear transformations can't capture this complexity. You need something that can learn the curves, corners, and complex decision boundaries that comes with modeling language in a higher dimensional space.

So how do we create this non-linearity? Simple - we use a non-linear function!

# What Makes a Function Non-Linear?

(Okay, I'll admit that I had to dig this out of Wikipedia but I swear I double checked it. )

A function $f$ is **linear** if it satisfies two properties:

1. **Additivity:** $f(x + y) = f(x) + f(y)$
2. **Homogeneity:** $f(a \cdot x) = a \cdot f(x)$

Matrix multiplication is linear because it satisfies both:

- $(x + y) \cdot W = x \cdot W + y \cdot W$
- $(a \cdot x) \cdot W = a \cdot (x \cdot W)$

We saw this in the first section where we could our two weight matrices into one. A function is **non-linear** if it breaks at least one of these properties.

Let's look at a simple non-linear function: $f(x) = x^2$

**Test additivity:**

$$
f(2 + 3) = 5^2 = 25
$$

$$
f(2) + f(3) = 4 + 9 = 13
$$

$$
25 \neq 13 \text{ ✗}
$$

Easy enough! This function is non-linear!

# Breaking Linearity

Now that we have the background context, we can get to activation functions.

An **activation function** is a non-linear function applied element-wise to the output of each layer. Remember that when we multiply an input vector by a matrix, we get another vector as output. "Element-wise" means we apply the activation function to each element in that output vector individually.

For example, if our layer outputs the vector `[2, -1, 3]`, and our activation function is $\sigma$, we compute:

$$
\sigma([2, -1, 3]) = [\sigma(2), \sigma(-1), \sigma(3)]
$$

Each element gets transformed independently by the same function.

So how does an activation function break linearity? Let's go through it.

In the first section, we combined our two equations and got:

$$
\hat{y} = (x \cdot W_1) \cdot W_2
$$

Now let's insert a non-linear function $\sigma$ (the activation):

$$
h_1 = \sigma(x \cdot W_1)
$$

$$
\hat{y} = \sigma(h_1 \cdot W_2)
$$

Even though $\sigma$ operates element-wise on individual vectors, when you try to combine the entire computation (two matmuls with two activations) into a single equivalent operation, you can't. There's no single matrix $W$ and single activation that can replicate:

$$
\sigma(\sigma(x \cdot W_1) \cdot W_2) \neq \sigma(x \cdot W)
$$

The activations are independent operations on each layer's output, but that's exactly what prevents the layers from collapsing into one. Adding a non-linear step in between breaks that compatibility. You can't "undo" or "skip over" the activation function to combine the matrices, it's permanently inserted into the computation chain.

Let's see the most common activation functions and then talk about how activation functions preventing layer collapse drives learing.

## ReLU (Rectified Linear Unit)

The simplest and most popular activation function:

$$
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$

Graphically:

![relu](/activations/relu.png)

**What it does:** Zeroes out negative values, keeps positive values unchanged.

Let's see it in action with numbers:

$$
x = [-2, -1, 0, 1, 2]
$$

$$
\text{ReLU}(x) = [0, 0, 0, 1, 2]
$$

**Why it works:**

- **Sparsity**: About half the neurons are zero (inactive), making the network efficient
- **Simple gradient**: Derivative is either 0 or 1, making backpropagation fast
- **Non-saturating**: Unlike older activations (sigmoid, tanh), ReLU doesn't "saturate" for large inputs

**The intuition:** ReLU acts like a gate. If a neuron's activation is negative (not relevant), shut it off. If positive (relevant), let it through. This creates **selective pathways** through the network.

## GELU (Gaussian Error Linear Unit)

A smoother, more sophisticated version of ReLU:

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

Where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution. In practice, it's approximated as:

$$
\text{GELU}(x) \approx 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right]\right)
$$

Graphically:

![gelu](/activations/gelu.png)

**What it does:** Similar to ReLU but with smooth transitions instead of a hard cutoff at zero.

Example:

$$
x = [-2, -1, 0, 1, 2]
$$

$$
\text{GELU}(x) \approx [-0.04, -0.16, 0, 0.84, 1.96]
$$

Notice that small negative values get small negative outputs (not zero like ReLU). This "soft gating" preserves more information.

**Why it works:**

- **Smooth gradients**: Better for optimization (no sudden jumps)
- **Probabilistic interpretation**: It's like asking "what's the probability this neuron should be active?"
- **Better performance**: Empirically outperforms ReLU on large language models

**The intuition:** GELU is like a probabilistic gate. Instead of a hard on/off switch, it asks "how confident am I that this feature is relevant?" and scales the output accordingly.

<Note>

Modern LLMs like GPT-3, GPT-4, and LLaMA all use GELU or variants of it. The smooth gradients help with training stability at massive scale.

</Note>

## SiLU / Swish (Sigmoid Linear Unit)

Another smooth alternative:

$$
\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

Where $\sigma(x)$ is the sigmoid function.

Example:

$$
x = [-2, -1, 0, 1, 2]
$$

$$
\text{SiLU}(x) \approx [-0.24, -0.27, 0, 0.73, 1.76]
$$

**Why it works:**

- Even smoother than GELU
- Self-gated: the input modulates itself
- Used in some vision models and efficient architectures

# How Activation Functions Enable Learning

Let's see concretely how activations let us learn non-linear patterns.

Suppose we want to learn the **XOR function** (exclusive or):

| Input ($x_1$, $x_2$) | Output |
| -------------------- | ------ |
| (0, 0)               | 0      |
| (0, 1)               | 1      |
| (1, 0)               | 1      |
| (1, 1)               | 0      |

This is the classic example of a **non-linearly separable** function. You can't draw a single straight line to separate the 1s from the 0s.

![xor](/activations/xor_problem.png)

**A linear model fails:**

No matter what weight matrix $W$ you use, $y = x \cdot W$ can't solve XOR. It can only produce linear decision boundaries.

**A network with activations succeeds:**

Let's use a 2-layer network with ReLU:

$$
h = \text{ReLU}(x \cdot W_1)
$$

$$
y = h \cdot W_2
$$

With learned weights:

$$
W_1 = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}, \quad W_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}
$$

Let's trace through input $(1, 1)$:

**Layer 1:**

$$
h = \text{ReLU}([1, 1] \cdot \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}) = \text{ReLU}([2, 2]) = [2, 2]
$$

**Layer 2:**

$$
y = [2, 2] \cdot \begin{bmatrix} 1 \\ -2 \end{bmatrix} = 2 - 4 = -2 \approx 0 \text{ ✓}
$$

The ReLU activation created a **non-linear decision boundary** that separates the XOR classes.

**The key insight:** Activations let each layer create **new feature spaces** where previously inseparable patterns become separable.

# From XOR to Language Understanding

The XOR example seems trivial, but the principle scales to language.

Consider understanding negation: "not good" vs "not bad"

- "good" → positive sentiment
- "not good" → negative sentiment
- "bad" → negative sentiment
- "not bad" → positive sentiment (!)

This is non-linear! The word "not" doesn't just shift sentiment by a fixed amount—it reverses it based on context.

**How activations help:**

After several layers with activations:

1. **Layer 1-20**: Learn that "not" is a modifier
2. **Layer 21-40**: Learn that "not" near sentiment words reverses polarity
3. **Layer 41-60**: Learn that "not bad" is idiomatic (means "good", not just "not negative")
4. **Layer 61-80**: Learn when this pattern applies vs when "not bad" means "terrible"

Each layer with its activation function creates a new feature space where these increasingly complex patterns become learnable via linear transformations.

Without activations, the model would be stuck with: "not" + "good" = some fixed weighted sum. It couldn't learn the conditional, context-dependent reversal.

# Why Not Just Use One Big Activation?

You might ask: why put activations between **every** layer? Why not just:

$$
y = \sigma(x \cdot W_1 \cdot W_2 \cdot W_3 \cdot ... \cdot W_{96})
$$

Because then you're back to a single linear transformation! Remember, all those matmuls collapse:

$$
y = \sigma(x \cdot W_{\text{combined}})
$$

This is just **one** non-linear transformation. It's slightly better than pure linear, but nowhere near as expressive as:

$$
y = \sigma_{96}(\sigma_{95}(...\sigma_2(\sigma_1(x \cdot W_1) \cdot W_2)...) \cdot W_{96})
$$

Each $\sigma$ creates a new opportunity to reshape the feature space. With 96 activations, you get 96 chances to create increasingly abstract representations.

**Analogy:** It's like sculpting clay:

- One activation = one reshaping of the clay
- 96 activations = 96 reshapings, each building on the previous refinement

By the end, you've sculpted something incredibly intricate that one reshaping could never achieve.

# The Universal Approximation Theorem

Here's the mathematical justification: the **Universal Approximation Theorem** states that a neural network with:

- At least one hidden layer
- A non-linear activation function
- Enough neurons

Can approximate **any continuous function** to arbitrary precision.

This is huge! It means neural networks with activations are theoretically capable of learning any pattern in data, no matter how complex.

**The catch:** You need non-linearity. Without activation functions, the theorem doesn't hold. You're stuck with linear functions, which can only approximate... other linear functions.

**In practice:** GPT-4 with 96 layers and GELU activations can approximate incredibly complex functions like:

- "Given this context, what word comes next?"
- "Given this question, what's a coherent answer?"
- "Given this code, where's the bug?"

Each layer with its activation refines the approximation, building from simple patterns (layer 1: "this is a noun") to complex reasoning (layer 90: "given the conversation context, this answer would be most helpful").

# Wrapping Up

Activation functions are the difference between a calculator and intelligence.

Matrix multiplication gives us efficient, learnable transformations. But it's **activations** that let those transformations build on each other, creating the hierarchical, non-linear feature extraction that makes modern AI possible.

Without ReLU, GELU, or their cousins:

- Your 96-layer network is really just 1 layer in disguise
- You can't learn XOR, let alone language
- ChatGPT would be an expensive linear regression

The magic of neural networks isn't just the depth or the parameters—it's the interleaving of linear transformations (matmul) and non-linear activations that unlocks their power.

Each matmul rotates and scales the feature space.
Each activation reshapes it non-linearly.
Do this 96 times, and you get something that can understand and generate human language.

Pretty wild that $\max(0, x)$ is part of what makes ChatGPT possible, huh?

Until next time!

Evis

---

**Further Reading:**

- My previous post on matrix multiplication and meaning
- The original ReLU paper (Nair & Hinton, 2010)
- GELU paper (Hendrycks & Gimpel, 2016)
- Universal Approximation Theorem (Cybenko, 1989)
