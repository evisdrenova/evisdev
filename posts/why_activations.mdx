---
title: "From Activations to Meaning"
date: "2025-10-07"
publish: false
---

import Note from "../components/Note";

In my last post, [From Matmul to Meaning](/why_matmul), I explained how matrix multiplication is the fundamental operation that lets LLMs understand meaning. We saw how billions of matmuls transform word vectors through semantic space until they produce coherent responses.

At the end of that post, I mentioned activation functions as a critical piece I didn't cover (mainly because the blog was already getting long) but without activation functions, matrix multiplication can't learn anything interesting.

Even a 96-layer LLM like GPT-4 would collapse into a single matrix multiplication. How is this possible? How can 96 layers of transformations collapse into one?

Let's start at the beginning and build our way to the intuition.

# The Problem With Matrix Multiplication

Let's say we have a simple 2-layer neural network.

$$
h_1 = x \cdot W_1
$$

$$
\hat{y} = h_1 \cdot W_2
$$

Where $x$ is our input (our vector), $W_1$ and $W_2$ are weight matrices, and $h_1$ is the hidden layer and $\hat{y}$ is the output layer.

Visually, this looks like:

![ve](/whyactivation/nn.png)

<Note>

The weights in this diagram are the connections (lines) between nodes. The lines from the input to the hidden layer represent the W₁ matrix, and the lines from hidden to output represent the W₂ matrix. Each line is one weight value, and together they form the complete weight matrices.

We can represent this matrix like this:

$$
W_1 = \begin{bmatrix}
w_{x_1 h_1} & w_{x_1 h_2} & w_{x_1 h_3} \\
w_{x_2 h_1} & w_{x_2 h_2} & w_{x_2 h_3} \\
w_{x_3 h_1} & w_{x_3 h_2} & w_{x_3 h_3}
\end{bmatrix}
$$

Where the $w_{x_1 h_1}$ represents the weight going from the ${x_1}$ node to the ${h_1}$ node.

</Note>

We can substitute the first equation into the second:

$$
h_2 = (x \cdot W_1) \cdot W_2 = x \cdot (W_1 \cdot W_2)
$$

By the **associative property** of matrix multiplication, we can combine $W_1 \cdot W_2$ into a single matrix $W$:

$$
h_2 = x \cdot W
$$

Our "2-layer" network just collapsed into a single matrix multiplication!

Let's see this with actual numbers. Suppose:

$$
x = [1, 2], \quad W_1 = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad W_2 = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}
$$

**Layer 1:**

$$
h_1 = [1, 2] \cdot \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} = [4, 7]
$$

**Layer 2:**

$$
h_2 = [4, 7] \cdot \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} = [18, 15]
$$

Now let's compute $W_1 \cdot W_2$ directly:

$$
W = W_1 \cdot W_2 = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \cdot \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 5 \\ 7 & 5 \end{bmatrix}
$$

**Single layer:**

$$
h_2 = [1, 2] \cdot \begin{bmatrix} 4 & 5 \\ 7 & 5 \end{bmatrix} = [18, 15]
$$

Same answer! The two-layer network did nothing that a single layer couldn't do.

This is called **linearity**, and it's a fundamental limitation. No matter how many layers you stack, pure matrix multiplication can only represent **linear transformations**: rotations, scalings, and shears.

But language isn't linear. Relationships in language are complex, conditional, and context-dependent:

- "Bank" means different things near "river" vs "money"
- "Not bad" means "good" (negation reverses sentiment)
- "The animal didn't cross because it was tired" requires reasoning

**Linear transformations can't capture this complexity.**

# What Makes a Function Non-Linear?

A function $f$ is **linear** if it satisfies two properties:

1. **Additivity:** $f(x + y) = f(x) + f(y)$
2. **Homogeneity:** $f(a \cdot x) = a \cdot f(x)$

Matrix multiplication is linear because it satisfies both:

- $(x + y) \cdot W = x \cdot W + y \cdot W$
- $(a \cdot x) \cdot W = a \cdot (x \cdot W)$

A function is **non-linear** if it breaks at least one of these properties.

Let's look at a simple non-linear function: $f(x) = x^2$

**Test additivity:**

$$
f(2 + 3) = 5^2 = 25
$$

$$
f(2) + f(3) = 4 + 9 = 13
$$

$$
25 \neq 13 \text{ ✗}
$$

The function is non-linear! This "breaking" of linearity is exactly what we need.

# Activation Functions: Breaking Linearity

An **activation function** is a non-linear function applied element-wise to the output of each layer. Instead of:

$$
h_2 = (x \cdot W_1) \cdot W_2
$$

We insert a non-linear function $\sigma$ (the activation):

$$
h_1 = \sigma(x \cdot W_1)
$$

$$
h_2 = \sigma(h_1 \cdot W_2)
$$

Now we **can't** collapse the layers because $\sigma$ breaks the associative property:

$$
h_2 = \sigma(\sigma(x \cdot W_1) \cdot W_2)
$$

There's no way to simplify this into a single matrix! Each layer genuinely adds expressive power.

Let's see the most common activation functions.

## ReLU (Rectified Linear Unit)

The simplest and most popular activation function:

$$
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$

Graphically:

![relu](/activations/relu.png)

**What it does:** Zeroes out negative values, keeps positive values unchanged.

Let's see it in action with numbers:

$$
x = [-2, -1, 0, 1, 2]
$$

$$
\text{ReLU}(x) = [0, 0, 0, 1, 2]
$$

**Why it works:**

- **Sparsity**: About half the neurons are zero (inactive), making the network efficient
- **Simple gradient**: Derivative is either 0 or 1, making backpropagation fast
- **Non-saturating**: Unlike older activations (sigmoid, tanh), ReLU doesn't "saturate" for large inputs

**The intuition:** ReLU acts like a gate. If a neuron's activation is negative (not relevant), shut it off. If positive (relevant), let it through. This creates **selective pathways** through the network.

## GELU (Gaussian Error Linear Unit)

A smoother, more sophisticated version of ReLU:

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

Where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution. In practice, it's approximated as:

$$
\text{GELU}(x) \approx 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right]\right)
$$

Graphically:

![gelu](/activations/gelu.png)

**What it does:** Similar to ReLU but with smooth transitions instead of a hard cutoff at zero.

Example:

$$
x = [-2, -1, 0, 1, 2]
$$

$$
\text{GELU}(x) \approx [-0.04, -0.16, 0, 0.84, 1.96]
$$

Notice that small negative values get small negative outputs (not zero like ReLU). This "soft gating" preserves more information.

**Why it works:**

- **Smooth gradients**: Better for optimization (no sudden jumps)
- **Probabilistic interpretation**: It's like asking "what's the probability this neuron should be active?"
- **Better performance**: Empirically outperforms ReLU on large language models

**The intuition:** GELU is like a probabilistic gate. Instead of a hard on/off switch, it asks "how confident am I that this feature is relevant?" and scales the output accordingly.

<Note>

Modern LLMs like GPT-3, GPT-4, and LLaMA all use GELU or variants of it. The smooth gradients help with training stability at massive scale.

</Note>

## SiLU / Swish (Sigmoid Linear Unit)

Another smooth alternative:

$$
\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

Where $\sigma(x)$ is the sigmoid function.

Example:

$$
x = [-2, -1, 0, 1, 2]
$$

$$
\text{SiLU}(x) \approx [-0.24, -0.27, 0, 0.73, 1.76]
$$

**Why it works:**

- Even smoother than GELU
- Self-gated: the input modulates itself
- Used in some vision models and efficient architectures

# How Activation Functions Enable Learning

Let's see concretely how activations let us learn non-linear patterns.

Suppose we want to learn the **XOR function** (exclusive or):

| Input ($x_1$, $x_2$) | Output |
| -------------------- | ------ |
| (0, 0)               | 0      |
| (0, 1)               | 1      |
| (1, 0)               | 1      |
| (1, 1)               | 0      |

This is the classic example of a **non-linearly separable** function. You can't draw a single straight line to separate the 1s from the 0s.

![xor](/activations/xor_problem.png)

**A linear model fails:**

No matter what weight matrix $W$ you use, $y = x \cdot W$ can't solve XOR. It can only produce linear decision boundaries.

**A network with activations succeeds:**

Let's use a 2-layer network with ReLU:

$$
h = \text{ReLU}(x \cdot W_1)
$$

$$
y = h \cdot W_2
$$

With learned weights:

$$
W_1 = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}, \quad W_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}
$$

Let's trace through input $(1, 1)$:

**Layer 1:**

$$
h = \text{ReLU}([1, 1] \cdot \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}) = \text{ReLU}([2, 2]) = [2, 2]
$$

**Layer 2:**

$$
y = [2, 2] \cdot \begin{bmatrix} 1 \\ -2 \end{bmatrix} = 2 - 4 = -2 \approx 0 \text{ ✓}
$$

The ReLU activation created a **non-linear decision boundary** that separates the XOR classes.

**The key insight:** Activations let each layer create **new feature spaces** where previously inseparable patterns become separable.

# From XOR to Language Understanding

The XOR example seems trivial, but the principle scales to language.

Consider understanding negation: "not good" vs "not bad"

- "good" → positive sentiment
- "not good" → negative sentiment
- "bad" → negative sentiment
- "not bad" → positive sentiment (!)

This is non-linear! The word "not" doesn't just shift sentiment by a fixed amount—it reverses it based on context.

**How activations help:**

After several layers with activations:

1. **Layer 1-20**: Learn that "not" is a modifier
2. **Layer 21-40**: Learn that "not" near sentiment words reverses polarity
3. **Layer 41-60**: Learn that "not bad" is idiomatic (means "good", not just "not negative")
4. **Layer 61-80**: Learn when this pattern applies vs when "not bad" means "terrible"

Each layer with its activation function creates a new feature space where these increasingly complex patterns become learnable via linear transformations.

Without activations, the model would be stuck with: "not" + "good" = some fixed weighted sum. It couldn't learn the conditional, context-dependent reversal.

# Why Not Just Use One Big Activation?

You might ask: why put activations between **every** layer? Why not just:

$$
y = \sigma(x \cdot W_1 \cdot W_2 \cdot W_3 \cdot ... \cdot W_{96})
$$

Because then you're back to a single linear transformation! Remember, all those matmuls collapse:

$$
y = \sigma(x \cdot W_{\text{combined}})
$$

This is just **one** non-linear transformation. It's slightly better than pure linear, but nowhere near as expressive as:

$$
y = \sigma_{96}(\sigma_{95}(...\sigma_2(\sigma_1(x \cdot W_1) \cdot W_2)...) \cdot W_{96})
$$

Each $\sigma$ creates a new opportunity to reshape the feature space. With 96 activations, you get 96 chances to create increasingly abstract representations.

**Analogy:** It's like sculpting clay:

- One activation = one reshaping of the clay
- 96 activations = 96 reshapings, each building on the previous refinement

By the end, you've sculpted something incredibly intricate that one reshaping could never achieve.

# The Universal Approximation Theorem

Here's the mathematical justification: the **Universal Approximation Theorem** states that a neural network with:

- At least one hidden layer
- A non-linear activation function
- Enough neurons

Can approximate **any continuous function** to arbitrary precision.

This is huge! It means neural networks with activations are theoretically capable of learning any pattern in data, no matter how complex.

**The catch:** You need non-linearity. Without activation functions, the theorem doesn't hold. You're stuck with linear functions, which can only approximate... other linear functions.

**In practice:** GPT-4 with 96 layers and GELU activations can approximate incredibly complex functions like:

- "Given this context, what word comes next?"
- "Given this question, what's a coherent answer?"
- "Given this code, where's the bug?"

Each layer with its activation refines the approximation, building from simple patterns (layer 1: "this is a noun") to complex reasoning (layer 90: "given the conversation context, this answer would be most helpful").

# Wrapping Up

Activation functions are the difference between a calculator and intelligence.

Matrix multiplication gives us efficient, learnable transformations. But it's **activations** that let those transformations build on each other, creating the hierarchical, non-linear feature extraction that makes modern AI possible.

Without ReLU, GELU, or their cousins:

- Your 96-layer network is really just 1 layer in disguise
- You can't learn XOR, let alone language
- ChatGPT would be an expensive linear regression

The magic of neural networks isn't just the depth or the parameters—it's the interleaving of linear transformations (matmul) and non-linear activations that unlocks their power.

Each matmul rotates and scales the feature space.
Each activation reshapes it non-linearly.
Do this 96 times, and you get something that can understand and generate human language.

Pretty wild that $\max(0, x)$ is part of what makes ChatGPT possible, huh?

Until next time!

Evis

---

**Further Reading:**

- My previous post on matrix multiplication and meaning
- The original ReLU paper (Nair & Hinton, 2010)
- GELU paper (Hendrycks & Gimpel, 2016)
- Universal Approximation Theorem (Cybenko, 1989)
