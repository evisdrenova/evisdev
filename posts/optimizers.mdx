---
title: "The intuition behind how neural networks learn"
date: "2025-10-29"
publish: true
---

What does it mean for a neural network to "learn"?

When you train GPT or Claude or any neural network, what's _actually happening_ inside the model as it's learning? How does a bunch of matrix multiplications somehow figure out how to write code or classify images or predict the next word?

I wanted to write a technical yet intuitive guide that answers these questions from first principles. We'll walk through how loss functions, gradient descent, and optimizers work together to update model weights, the actual mechanics of learning.

We'll cover the math, but more importantly, we'll build the intuition for _why_ it all works.

Let's jump in.

# The Optimization Problem

Think about the last time that you learned something new. Maybe it was a new skill or recipe or instrument. Your first attempt? Probably not perfect. But each time you try again, you get a little closer. You think about what you need to do, make adjustments, tweak the timing and eventually you nail it.

Neural networks actually learn in a similar way. We can train a neural network to learn something, say how to classify an email as spam, and over a number of repititions (we call them epochs), they gradually improve until we're happy with the results.

Here's the process:

1. The network makes a prediction
2. We measure how far off it was from the correct answer, if we're happy then we stop training, otherwise, we continue
3. It adjusts its internal parameters to do better next time
4. Repeat

This process relies on two fundamental concepts:

1. Measuring how far off we are from where we want to be. We call this measure a **loss function**. The goal is to make this distance as small as possible so we are as close to our ideal state as possible. In the email spam example above, this would mean being able to perfectly classify every spam email as spam and never missing one. In reality, we're never perfect but we try to get as close as we can.

2. Updating our model weights to reduce the loss function next time we try again. This is done by an algorithm called an **optimizer**. Optimizers decide _how_ to adjust weights in order to reduce the loss. This means determinining which direction to move, how big a step to take, and when to speed up or slow down.

At its core, training a neural network is an optimization problem. We have a loss function, let's call it: $L(θ)$ that measures how wrong our model is, and we want to find parameters (weights) $θ$ that minimize this loss.

![pro](/optimizers/process.png)

But, first, how do we measure the loss?

# How Wrong Are We?

The loss function is our reality check—it tells us exactly how far our model's predictions are from the truth. But "how wrong" can mean different things depending on what we're trying to do.

### Common Loss Functions

**For Classification Problems**

When we're sorting things into categories (like spam vs. not spam), we typically use **cross-entropy loss**. Think of it like a confidence penalty: if your model says "I'm 95% sure this is spam" and it actually is spam, the loss is very small. But if it says "I'm 95% sure this is NOT spam" and it turns out to be spam? The loss shoots up dramatically.

Mathematically, for binary classification (two categories), it looks like this:

```
L = -[y log(ŷ) + (1-y) log(1-ŷ)]
```

Where `y` is the actual label (0 or 1) and `ŷ` is our model's predicted probability.

**For Regression Problems**

When we're predicting continuous values (like house prices or temperatures), we often use **mean squared error (MSE)**. This one's intuitive: take the difference between your prediction and the actual value, square it (so negative errors don't cancel out positive ones), and average across all examples:

```
L = (1/n) Σ(y - ŷ)²
```

If you predicted a house would cost $300,000 but it actually sold for $350,000, that $50,000 error gets squared to 2.5 billion, making it a pretty significant contribution to your loss.

### Why Squaring Matters

Notice how MSE squares the errors? This isn't arbitrary. Squaring does two important things:

- **Penalizes large errors more heavily**: Being off by 10 is worse than being off by 5 twice
- **Makes the math work beautifully**: Squared errors have nice mathematical properties that make optimization smoother

### The Loss Landscape

Here's where it gets interesting. Imagine your loss function as a landscape—a hilly terrain where the height at any point represents how wrong your model is with those particular weights. Training a neural network is like trying to find the lowest valley in this landscape.

For simple models, this landscape might be a smooth bowl—there's one obvious lowest point. But for deep neural networks with millions of parameters? The landscape is extraordinarily complex, with countless hills, valleys, and saddle points in millions of dimensions.

![loss-landscape](/optimizers/landscape.png)

Our job is to navigate this landscape efficiently, finding the lowest point (or at least a very low one) without having to check every possible location. That's where optimizers come in.

So now that we can measure how wrong we are at any given point, the natural question becomes: how do we use this information to get better?

## The Foundation: Gradient Descent

**The Mathematics**

The simplest approach is gradient descent:

```
θ_(t+1) = θ_t - α · ∇L(θ_t)
```

Where:

- `θ_t` are the parameters at time step t
- `α` is the learning rate (step size)
- `∇L(θ_t)` is the gradient of the loss with respect to parameters

**The Intuition**

Imagine you're blindfolded on a hilly landscape, trying to find the lowest point. You can only feel the slope beneath your feet. The gradient tells you which direction slopes downward most steeply. You take a step in that direction. The learning rate (`α`) determines how big that step is.

**The Problem**

Standard gradient descent has three critical issues:

1. **Computationally expensive**: Computing gradients over the entire dataset is slow. If you have a million training examples, you need to process all of them before taking a single step.

2. **Fixed learning rate**: Same step size everywhere, even when the terrain varies dramatically. Sometimes you're on a gentle slope and could take big steps. Other times you're near the minimum and should take tiny steps.

3. **No memory**: Each step completely forgets previous steps, leading to oscillation. If the loss surface is shaped like a narrow valley, you'll bounce back and forth between the walls while making slow progress forward.

This is where things get interesting.

---

## Stochastic Gradient Descent (SGD)

**The Mathematics**

Instead of computing gradients over the entire dataset, compute them on mini-batches:

```
θ_(t+1) = θ_t - α · ∇L(θ_t; x^(i:i+n))
```

Where `x^(i:i+n)` is a mini-batch of data.

**The Intuition**

Instead of carefully measuring the slope of the entire mountain before each step, you take quick measurements of small patches and move based on those. It's noisy, but much faster.

Think of it like polling: you don't need to survey every voter to understand the election—a representative sample gives you a good estimate. Similarly, a mini-batch gives you a noisy but useful estimate of the true gradient.

**The Trade-off**

You introduce noise (each mini-batch gives a slightly different gradient), but you can take many more steps in the same time. With a million training examples:

- Gradient descent: 1 update per pass through all data
- SGD with batch size 32: ~31,000 updates per pass through all data

The noise usually helps! It acts as a form of regularization and helps escape sharp local minima.

---

## Adding Momentum: Remembering the Past

**The Problem SGD Solves Poorly**

Picture a ball rolling through a narrow valley—steep walls but a gentle slope toward the minimum. SGD will bounce wildly between the walls (high gradient in that direction) while making painfully slow progress forward (low gradient in that direction).

This happens because SGD has no memory. Each update is based solely on the current gradient.

**The Mathematics**

Momentum adds a velocity term:

```
v_t = γ · v_(t-1) + α · ∇L(θ_t)
θ_(t+1) = θ_t - v_t
```

Where `γ` (typically 0.9) is the momentum coefficient.

Expanded recursively:

```
v_t = α · g_t + γ · α · g_(t-1) + γ² · α · g_(t-2) + γ³ · α · g_(t-3) + ...
```

This is an exponentially weighted moving average of past gradients, where recent gradients have more influence.

**The Intuition**

Your parameter update now has **inertia**. Think of a ball rolling downhill:

- If gradients consistently point in the same direction, you build up speed (like rolling down a long slope)
- If they oscillate, the momentum dampens the zigzagging (like friction smoothing out the path)
- You can roll through shallow local minima instead of getting stuck

**Why It Works: The Memory Effect**

The `γ` parameter controls how much history you remember:

- `γ = 0`: No momentum (regular SGD)—you instantly forget everything
- `γ = 0.9`: Roughly 10 steps of memory (`1/(1-0.9) ≈ 10`)
- `γ = 0.99`: Roughly 100 steps of memory
- `γ → 1`: Infinite memory—you never slow down

**Visual Example**

Consider optimizing through this valley:

```
     |  ↓↓↓  |
     | ↓ ↓ ↓ |
     |↓  ↓  ↓|
     |   ↓   |  ← minimum
     |_______|
```

- **Without momentum**: Bounces between walls ← → ← → ← →, slow descent
- **With momentum**: Dampened oscillations ← → ← →, faster descent ↓↓↓

The vertical component (toward minimum) accumulates, while horizontal components (between walls) cancel out.

---

## Adaptive Learning Rates: Different Speeds for Different Parameters

**The Core Insight**

Not all parameters should update at the same rate. Consider three scenarios:

1. **Sparse features**: The word "antidisestablishmentarianism" appears rarely in your training data. When it does appear, you should make a large update to its embedding—you don't have many chances to learn from it.

2. **Different scales**: One parameter might have gradients around 0.001, another around 100. Using the same learning rate for both is suboptimal.

3. **Different curvatures**: The loss surface might be steep in one direction (converge quickly) but flat in another (need larger steps).

The solution? Make the learning rate **adaptive** per parameter.

### AdaGrad: The First Adaptive Method

**Mathematics**:

```
G_t = G_(t-1) + g_t²  (accumulate squared gradients)
θ_(t+1) = θ_t - α · g_t / (√G_t + ε)
```

**The Intuition**

Divide each parameter's learning rate by the square root of the sum of all its past squared gradients.

- Large historical gradients → `√G_t` is large → learning rate becomes small
- Small historical gradients → `√G_t` is small → learning rate stays large

Think of it as: "If this parameter has been making big updates, slow down. If it's been making small updates, speed up."

**Why square the gradients?** We want to measure magnitude regardless of direction. Squaring makes negative and positive gradients contribute equally.

**The Problem**

`G_t` only accumulates—it never decreases. Over time, `√G_t` grows and grows, making the learning rate smaller and smaller until learning effectively stops. This is called the "decaying learning rate problem."

### RMSProp: Fixing AdaGrad

**Mathematics**:

```
v_t = β · v_(t-1) + (1 - β) · g_t²
θ_(t+1) = θ_t - α · g_t / (√v_t + ε)
```

**The Intuition**

Instead of accumulating all squared gradients forever, maintain an exponentially decaying average. Recent gradient magnitudes matter more than ancient history.

With `β = 0.9`, gradients from 10 steps ago have only 10% of their original influence. Gradients from 100 steps ago are essentially forgotten.

**The Key Difference**

- **AdaGrad**: Infinite memory → learning rate can only decrease
- **RMSProp**: Finite memory → learning rate can increase or decrease based on recent behavior

This solves the decaying learning rate problem while keeping the benefits of adaptive per-parameter learning rates.

---

## Adam: Putting It All Together

**The Philosophy**

What if we combined everything we've learned?

1. **Momentum** (from SGD + Momentum): Remember the direction we've been going
2. **Adaptive learning rates** (from RMSProp): Scale updates per parameter
3. **One more thing**: Fix an initialization bias we'll discover

This is Adam: **Ada**ptive **M**oment Estimation.

**The Mathematics**

Adam maintains two moving averages—one for gradients (momentum), one for squared gradients (adaptive learning rates):

**First moment (mean of gradients)**:

```
m_t = β₁ · m_(t-1) + (1 - β₁) · g_t
```

**Second moment (uncentered variance of gradients)**:

```
v_t = β₂ · v_(t-1) + (1 - β₂) · g_t²
```

**Bias correction** (we'll see why this matters):

```
m̂_t = m_t / (1 - β₁^t)
v̂_t = v_t / (1 - β₂^t)
```

**Parameter update**:

```
θ_(t+1) = θ_t - α · m̂_t / (√v̂_t + ε)
```

**Breaking Down the Update Rule**

Let's parse this carefully:

```
θ_(t+1) = θ_t - α · m̂_t / (√v̂_t + ε)
```

- **Numerator (`m̂_t`)**: Where to go—the momentum-smoothed direction
- **Denominator (`√v̂_t`)**: How much to scale the step—adaptive per parameter
- **Their ratio**: A normalized update that's scaled by past gradient variance

**Three Ways to Understand Adam**

**Perspective 1: Adaptive Step Sizes**

Look at what happens for different parameters:

Parameter with large, consistent gradients:

- `m̂_t` is large (strong signal)
- `v̂_t` is large (consistent magnitude)
- `m̂_t / √v̂_t` → moderate update (doesn't overshoot)

Parameter with small, noisy gradients:

- `m̂_t` is small (weak signal)
- `v̂_t` is small (low magnitude)
- `m̂_t / √v̂_t` → moderate update (doesn't under-step)

Adam automatically balances these!

**Perspective 2: Signal-to-Noise Ratio**

Think of `m̂_t` as the signal (consistent direction) and `√v̂_t` as the noise (variability):

```
update ∝ signal / noise
```

- High signal-to-noise → confident, take larger steps
- Low signal-to-noise → uncertain, take smaller steps

This is like listening to someone speak: if the signal is clear above the noise, you hear them well. If it's noisy, you're less confident in what you heard.

**Perspective 3: Second-Order Approximation**

The division by `√v̂_t` approximates second-order optimization (Newton's method) without computing expensive second derivatives.

Newton's method uses: `θ_(t+1) = θ_t - H^(-1) · g_t`, where `H` is the Hessian (matrix of second derivatives).

Adam approximates the Hessian's inverse using only gradient information: `√v̂_t` acts like a diagonal approximation of the Hessian.

**Why Bias Correction Is Critical**

We initialize `m_0 = 0` and `v_0 = 0`. Without correction, here's what happens in iteration 1:

```
m_1 = 0.9 · 0 + 0.1 · g_1 = 0.1 · g_1
v_1 = 0.999 · 0 + 0.001 · g_1² = 0.001 · g_1²
```

Both are severely biased toward zero! Our estimates are 10x and 1000x too small.

With bias correction:

```
m̂_1 = 0.1 · g_1 / (1 - 0.9¹) = 0.1 · g_1 / 0.1 = g_1
v̂_1 = 0.001 · g_1² / (1 - 0.999¹) = 0.001 · g_1² / 0.001 = g_1²
```

Perfect! The correction factors `(1 - β₁^t)` and `(1 - β₂^t)` exactly cancel the bias.

**What happens over time?**

- Iteration 1: Correction factors are ~0.1 and ~0.001 (large correction needed)
- Iteration 10: Correction factors are ~0.65 and ~0.01 (moderate correction)
- Iteration 1000: Correction factors are ~1.0 and ~1.0 (barely any correction)

The correction automatically fades as the estimates become reliable.

**Hyperparameters and Their Meaning**

Default values:

- `α = 0.001`: Global learning rate
- `β₁ = 0.9`: First moment decay (momentum memory)
- `β₂ = 0.999`: Second moment decay (variance memory)
- `ε = 1e-8`: Numerical stability constant

**Why these specific values?**

`β₁ = 0.9` gives roughly 10 steps of memory for direction:

```
Effective memory = 1/(1-β₁) = 1/(1-0.9) = 10
```

`β₂ = 0.999` gives roughly 1000 steps of memory for variance:

```
Effective memory = 1/(1-β₂) = 1/(1-0.999) = 1000
```

**Why longer memory for variance?** Estimating variance requires more samples than estimating mean. You need to see more data to confidently say "this parameter's gradients are consistently large" versus "this direction is generally downhill."

---

## Memory and Computational Cost

**Storage Requirements**

For a model with N parameters:

| Optimizer      | Memory Overhead | What's Stored                        |
| -------------- | --------------- | ------------------------------------ |
| SGD            | 0               | Nothing                              |
| SGD + Momentum | N               | Velocity `v`                         |
| AdaGrad        | N               | Accumulated `G`                      |
| RMSProp        | N               | Moving average `v`                   |
| Adam           | 2N              | First moment `m` + second moment `v` |

**Is 2N a problem?**

For a model with 100M parameters:

- Parameters: 400 MB (float32)
- Adam state: 800 MB additional
- **Total**: 1.2 GB

Compare to:

- Activations during training (batch=32): Often 5-10 GB
- Gradients: 400 MB

Memory is rarely the bottleneck for Adam. The activations dominate.

**Computational Cost Per Step**

All modern optimizers have O(N) cost per step. The differences are minimal:

- **SGD**: 1 multiply, 1 add per parameter → 2 ops
- **Adam**: 2 EMA updates, 2 bias corrections, 1 sqrt, 2 divides → ~10 ops

In practice, this overhead is negligible ( less than 1% of total training time) because:

1. Forward and backward passes dominate (90%+ of time)
2. Modern hardware handles element-wise operations efficiently
3. The optimizer often runs on CPU while GPU does forward/backward

---

## Adam Variants and Improvements

### AdamW: Decoupled Weight Decay

**The Problem**

In standard Adam, L2 regularization interacts poorly with adaptive learning rates.

With L2 regularization, we modify the loss:

```
L'(θ) = L(θ) + (λ/2) · ||θ||²
```

Taking gradients:

```
∇L'(θ) = ∇L(θ) + λ · θ
```

This adds `λ·θ` to the gradient, which then gets adapted by Adam's learning rate scaling. This means:

- Parameters with large historical gradients get less weight decay (because their effective learning rate is smaller)
- Parameters with small historical gradients get more weight decay (because their effective learning rate is larger)

This is backwards! We want consistent regularization.

**AdamW Solution**

Decouple weight decay from gradient-based optimization:

```
θ_(t+1) = θ_t - α · m̂_t / (√v̂_t + ε) - α · λ · θ_t
                                         ^^^^^^^^^^^
                                    weight decay applied
                                    after adaptation
```

**The Intuition**

Apply regularization directly to parameters, not through gradients. This makes regularization strength consistent across all parameters regardless of their gradient history.

**When to use**: AdamW often generalizes better than Adam, especially for transformers and language models. It's rapidly becoming the default.

### AMSGrad: Fixing Non-Convergence

**The Problem**

In some pathological cases, Adam can fail to converge because `v_t` can decrease, causing the effective learning rate to increase when it shouldn't.

Example: If recent gradients are smaller than historical gradients, `v_t` decreases, `√v̂_t` decreases, and the effective learning rate (denominator gets smaller) increases. This can cause instability.

**Solution**

Use the maximum of all past `v̂_t` values:

```
v̂_t_max = max(v̂_(t-1)_max, v̂_t)
θ_(t+1) = θ_t - α · m̂_t / (√v̂_t_max + ε)
```

**The Intuition**

Never let the adaptive learning rate increase—only decrease or stay the same. This guarantees monotonic convergence in convex settings.

**In practice**: Rarely makes a difference for neural networks (which are highly non-convex). Most practitioners stick with standard Adam.

### NAdam: Nesterov Momentum

**The Idea**

Combine Adam with Nesterov accelerated gradient.

Standard momentum: Look at where you are, then jump

```
Evaluate gradient at current position → Jump
```

Nesterov momentum: Jump first, then look

```
Jump → Evaluate gradient at new position → Adjust
```

**The Intuition**

Nesterov momentum is slightly more "optimistic." It assumes the momentum will carry you forward, evaluates the gradient at that future point, then makes a correction.

Think of it like steering a car: Nesterov steers where you're about to be, not where you currently are.

The math gets complicated, but the intuition is that Nesterov can converge faster by being slightly more aggressive in consistent directions.

### RAdam: Rectified Adam

**The Problem**

Early in training (first few iterations), the variance estimate `v̂_t` is very unreliable because it's based on so few samples. This can cause large, erratic updates.

**Solution**

Add a rectification term that gradually ramps up the adaptive learning rate based on the variance estimate's reliability:

```
ρ_t = ρ_∞ - 2·t·β₂^t / (1 - β₂^t)
```

Where `ρ_∞` is the maximum length of the approximated SMA (Simple Moving Average).

If `ρ_t > 4`, use Adam. Otherwise, fall back to SGD with momentum.

**The Intuition**

Don't trust the variance estimate until you have enough samples. In the very beginning, just use momentum without the adaptive part.

**When to use**: When training is unstable in the first few epochs, or when using very small learning rates.

---

## When Adam Fails: The Generalization Gap

**The Surprising Weakness**

Despite its popularity, Adam has a well-documented issue: it sometimes generalizes worse than SGD, particularly on vision tasks.

**Empirical Observation**:

- **Adam**: Lower training loss, faster convergence, sometimes worse test accuracy
- **SGD**: Higher training loss, slower convergence, sometimes better test accuracy

Why would a slower optimizer that achieves worse training loss generalize better?

**Hypothesis 1: Sharp vs. Flat Minima**

Imagine two valleys at the same depth:

```
Valley A (Sharp):        Valley B (Flat):
    \  /                     \___/
     \/
```

Both have the same loss, but:

- **Sharp minimum**: Small perturbations (like test data) push you up the walls → high test loss
- **Flat minimum**: Small perturbations barely change the loss → robust test performance

**The claim**: Adam's adaptive learning rates allow it to converge into sharp minima. SGD's constant learning rate acts as implicit regularization, preventing convergence to sharp minima—it naturally finds flatter basins.

**Why does this happen?**

Adam can take small, precise steps into narrow valleys because it adapts per parameter. SGD with its fixed learning rate "can't fit" into sharp minima—the noise keeps kicking it out.

**Hypothesis 2: Effective Learning Rate Decay**

In SGD, as you approach a minimum, gradients naturally get smaller:

```
Near minimum: ||∇L|| → 0
Effective step size: α · ||∇L|| → 0
```

This natural annealing helps convergence.

In Adam, the adaptive term `1/√v̂_t` can prevent this. If gradients are getting smaller, `v̂_t` also gets smaller, potentially keeping the effective learning rate high.

**Hypothesis 3: Different Loss Landscapes**

Vision models (CNNs) have highly non-convex loss surfaces with many good local minima. The optimizer's dynamics determine which basin of attraction you end up in.

Adam and SGD simply converge to different solutions. Adam finds one quickly (but sometimes sharp), SGD finds another slowly (but sometimes flat).

**Practical Guidance**

**Use Adam when**:

- Fast experimentation and iteration speed matter
- Working on NLP tasks (transformers strongly prefer Adam/AdamW)
- Training from scratch on large datasets
- You don't have time for extensive hyperparameter tuning

**Use SGD (+ momentum) when**:

- You need absolute best generalization on held-out data
- Training computer vision models (especially ResNets, EfficientNets)
- Fine-tuning pre-trained models
- You can afford to carefully tune learning rates and schedules

**The Hybrid Approach**:

1. Use Adam for initial exploration and architecture search (fast iteration)
2. Switch to SGD for final training once you've found a good architecture
3. Or use Adam with aggressive learning rate decay schedules

---

## Understanding Optimization Landscapes

**Visualizing the Journey**

Consider training a simple 2-parameter model. The loss landscape might look like:

```
        High Loss
           ↑
           │     ╱╲
           │    ╱  ╲    ╱╲
           │   ╱    ╲  ╱  ╲
           │  ╱  A   ╲╱ B  ╲
           │ ╱              ╲___
           │╱___________________╲→ Parameter 1
           │
        Parameter 2
```

- **Point A**: Sharp minimum—Adam might converge here quickly
- **Point B**: Flat minimum—SGD more likely to find this eventually

**Why Flat Minima Generalize Better**

Train on dataset D, test on dataset D'. If the distribution shifts even slightly:

- **Sharp minimum**: Performance degrades significantly (steep walls mean small changes → large loss increase)
- **Flat minimum**: Performance degrades gracefully (flat floor means small changes → small loss increase)

This is the essence of generalization: robustness to distribution shift.

**The Batch Size Connection**

Batch size interacts surprisingly with optimizer choice:

- **Small batches** (8-32): Noisy gradients → need adaptive learning rates → Adam wins
- **Large batches** (1024+): Accurate gradients → simple optimization works → SGD competitive

This explains why:

- Distributed training (large batches) often uses SGD
- Single-GPU training (smaller batches) often uses Adam

Large batches give you more accurate gradient estimates, reducing the need for Adam's adaptive learning rates to deal with noise.

**Learning Rate Schedules**

Both Adam and SGD benefit from learning rate schedules:

**Cosine Annealing**:

```
α_t = α_min + (α_max - α_min) · (1 + cos(πt/T)) / 2
```

Smoothly reduces learning rate from `α_max` to `α_min` following a cosine curve. This helps convergence to flat minima.

**Warmup**:

```
If t < warmup_steps:
    α_t = α_base · (t / warmup_steps)
Else:
    α_t = α_base
```

**Why warmup helps Adam**: Early in training, the second moment estimate `v̂_t` is unstable (based on few samples). Warmup gives it time to stabilize before taking aggressive steps.

Without warmup: Unreliable `v̂_t` → erratic effective learning rates → instability
With warmup: Gradual ramp-up → `v̂_t` accumulates information → stable convergence

---

## The Big Picture: Optimization as Search

**What We're Really Doing**

Neural network training is search through an enormous parameter space. For a model with 100M parameters, you're navigating a 100-million-dimensional landscape.

The optimization algorithm determines:

1. **What trajectory we take** through this space
2. **How fast we move** along that trajectory
3. **Where we end up** (which local minimum we converge to)

Different optimizers take radically different paths and find different solutions.

**The Fundamental Trade-offs**

There's no free lunch in optimization:

- **Speed vs. Stability**: Aggressive updates (large learning rate) converge fast but might overshoot and diverge
- **Adaptation vs. Consistency**: Adaptive learning rates help navigate varied terrain but can find sharp minima
- **Memory vs. Simplicity**: Maintaining state (momentum, variance) helps but uses memory and adds complexity
- **Training Loss vs. Generalization**: The absolute minimum of training loss doesn't always generalize best

Every optimizer makes different trade-offs.

**The Future of Optimization**

Current research directions:

1. **Second-order methods**: K-FAC, Shampoo—using curvature information more effectively
2. **Learned optimizers**: Meta-learning optimization algorithms themselves (learning to learn)
3. **Distribution-aware**: Optimizers that explicitly model the data distribution
4. **Federated learning**: Optimization across distributed, heterogeneous data sources
5. **Neural architecture search**: Optimizing both architecture and weights simultaneously

---

## Conclusion

Adam works because it combines three powerful ideas:

1. **Momentum**: Remember where you've been—build velocity in consistent directions
2. **Adaptive learning rates**: Different speeds for different parameters—navigate varied terrain
3. **Bias correction**: Handle initialization properly—don't let early estimates mislead you

The math might look intimidating with all the moving averages and correction terms, but the intuition is straightforward: **take smart steps by remembering the past and adapting to the terrain.**

Is Adam perfect? No. It sometimes finds solutions that don't generalize as well as SGD, particularly on vision tasks. The speed-generalization trade-off is real.

But Adam's combination of fast convergence, stability across problems, and minimal hyperparameter tuning makes it the default choice for most deep learning applications.

**The Bottom Line**

Start with Adam (or AdamW). For 95% of applications, it will serve you well. If you need the absolute best test performance and have time to carefully tune learning rates and schedules, consider SGD with momentum. But don't prematurely optimize—Adam's speed advantage during experimentation usually outweighs SGD's potential generalization edge.

Now go train some models!

---

## Further Reading

**Original Papers**:

- Kingma & Ba (2015): "Adam: A Method for Stochastic Optimization"
- Loshchilov & Hutter (2019): "Decoupled Weight Decay Regularization" (AdamW)
- Reddi et al. (2018): "On the Convergence of Adam and Beyond" (AMSGrad)

**Empirical Comparisons**:

- Wilson et al. (2017): "The Marginal Value of Adaptive Gradient Methods"
- Schmidt et al. (2021): "Descending through a Crowded Valley"

**Practical Guides**:

- Ruder (2016): "An Overview of Gradient Descent Optimization Algorithms"
- Stanford CS231n course notes on optimization

**Advanced Topics**:

- Martens & Grosse (2015): K-FAC (Kronecker-factored approximate curvature)
- Anil et al. (2020): "Scalable Second Order Optimization for Deep Learning"

---

_Thanks for reading! If you found this helpful, consider implementing Adam from scratch as an exercise—there's no better way to truly understand an algorithm than to build it yourself._
