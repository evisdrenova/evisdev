---
title: "Why MatMul is the Beating Heart of Transformers"
date: "2025-10-04"
publish: false
---

Over the past few months, I've worked on a few LLM projects, namely GPT-rs and llama2.rs. These are full re-writes of GPT-2 from Python to Rust and Llama2 from C to Rust. At this point, I feel reasonably comfortable with a fairly straightforward transformer architecture and how it works.

But I often find myself asking, _why_ does it work? Why is that matrix multiplying two matrices together is the _right_ operation and how does that lead to a system like ChatGPT giving us a coherent response to our prompts?

It's one thing to implement `matmul` it's another to deeply understand why it works.

I like to start most of my blogs from the beginning and assume the reader doesn't know much and build my way to the intuition. If you already know what `matmul` is then you might want to skip the first 1-2 sections.

# What is Matrix Multiplication?

If you've never had the pleasure of taking a linear algebra class then you might have never really encountered matrix operations.

## 1. The Core Intuition: Linear Combinations

At its essence, **matrix multiplication** is just a way to combine vectors linearly.

If you have an input vector $x \in \mathbb{R}^d$ and a weight matrix $W \in \mathbb{R}^{d \times k}$, the product $xW$ produces a new vector in $\mathbb{R}^k$.

- Each coordinate of the output is a **weighted sum of the input features**.
- Said differently: _matmul lets you learn new ‚Äúviews‚Äù of the data by mixing the old features_.

üí° **Intuition:** If $x$ is a word embedding (‚Äúking‚Äù), and $W$ is trained, the output $xW$ can rotate, scale, or stretch that embedding to emphasize semantics the model cares about (‚Äúroyalty‚Äù, ‚Äúmale‚Äù, etc.).

---

## 2. Geometry: Why Linear Maps Matter

Every matrix multiplication is a **linear transformation**: it preserves straight lines and the origin.

- Scaling ‚Üí change magnitude.
- Rotation ‚Üí change orientation.
- Projection ‚Üí drop dimensions.

In LLMs, this means: ‚Äútake the input embedding and re-map it into a space where the relationships we want (context, similarity, attention) become easier to see.‚Äù

üìâ Example: Plot a 2D toy embedding of words _cat, dog, car_. A well-trained $W$ could separate _animals_ vs _vehicles_ just by rotating and scaling.

---

## 3. MatMul in Neural Nets: Why Not Something Else?

Suppose instead of matmul, we only used elementwise operations ($\odot$).

- $x \odot w$ scales each input independently ‚Äî but there‚Äôs no mixing of features.
- You‚Äôd never let ‚Äúfeature 3‚Äù influence ‚Äúoutput dimension 1.‚Äù

**MatMul‚Äôs power** is cross-talk: every output dimension can use _all_ input features.
This mixing is why we can approximate arbitrary functions.

---

## 4. Attention = MatMul Everywhere

This is where the LLM magic comes in. Attention is **just matmul** three times:

1. $Q = XW_Q$, $K = XW_K$, $V = XW_V$ ‚Üí project inputs into Query, Key, Value spaces.
2. Attention weights = $\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$ ‚Üí another matmul.
3. Weighted sum of values = $\text{softmax}(QK^T)V$ ‚Üí another matmul.

Why matmul?

- $QK^T$ computes _all pairwise similarities_ between tokens in one shot.
- Multiplying by $V$ lets us ‚Äúblend‚Äù representations according to those similarities.

‚ö° **This is the core of why transformers work.** MatMul lets every token contextually remix information from every other token.

---

## 5. Universal Approximation: Layers of MatMul + Nonlinearity

One matmul is just a linear transformation ‚Äî not enough to model complex functions. But stacking them with nonlinearities (like GELU/ReLU) means:

- Linear ‚Üí nonlinear ‚Üí linear ‚Üí nonlinear ‚Ä¶
- By the **universal approximation theorem**, this is enough to approximate _any_ continuous function.

So: MatMul is the ‚Äúmixing engine,‚Äù nonlinearity is the ‚Äúcurve bender.‚Äù

---

## 6. Intuition Summary

- MatMul mixes features into new bases (why we can represent abstract meaning).
- MatMul computes relationships (why attention works).
- MatMul layers approximate any function (why LLMs scale).

In short: **MatMul is the bridge between raw vectors and emergent meaning.**

---

## 7. (Optional) A Worked Toy Example

Take a 3-word toy sequence: ["I", "love", "AI"].

- Map to embeddings $X \in \mathbb{R}^{3 \times d}$.
- Compute $Q, K, V$ with matmul.
- Show $QK^T$ ‚Üí which tokens attend to which.
- Show weighted sum of $V$.

Walk through the numbers (maybe with $d=2$) so readers see attention literally as matmuls.

---

üî• My suggestion: include little diagrams at each step (2D embeddings rotating, heatmap for $QK^T$, etc).

---

Would you like me to **flesh out the toy example with numbers** (so your readers can see how $QK^T$ literally calculates attention)? That could make the intuition really click.

Perfect ‚Äî let‚Äôs do a small, worked-through **toy attention example** with real numbers. That way, in your blog you can walk the reader step by step through how matrix multiplication is the operation that makes attention tick.

---

## Toy Example: "I love AI"

Let‚Äôs pretend we have a tiny embedding space with dimension $d=2$ (so we can keep the math human-readable). Our input is the sequence:
$X = \text{["I", "love", "AI"]}$

### Step 1. Embeddings

Assign small vectors to each token:

- "I" ‚Üí $x_1 = [1, 0]$
- "love" ‚Üí $x_2 = [0, 1]$
- "AI" ‚Üí $x_3 = [1, 1]$

So $X \in \mathbb{R}^{3 \times 2}$:

$$
X = \begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}
$$

---

### Step 2. Project into $Q, K, V$

We pick simple weight matrices (normally learned):

$$
W_Q = W_K = W_V =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

So $Q = XW_Q$, $K = XW_K$, $V = XW_V$ ‚Üí all equal to $X$ in this toy case.

---

### Step 3. Compute Attention Scores ($QK^T$)

Now we take all pairwise dot products between tokens:

$$
QK^T =
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 2
\end{bmatrix}
$$

Interpretation:

- Token "I" has high similarity (1) with itself, and (1) with "AI".
- "love" has high similarity with itself, and (1) with "AI".
- "AI" is similar to everyone (1, 1, 2).

---

### Step 4. Softmax the Scores

Apply softmax row-wise (pretend temperature = 1):

- For "I": $\text{softmax}([1,0,1]) \approx [0.42, 0.16, 0.42]$
- For "love": $\text{softmax}([0,1,1]) \approx [0.16, 0.42, 0.42]$
- For "AI": $\text{softmax}([1,1,2]) \approx [0.21, 0.21, 0.58]$

So attention weights =

$$
A =
\begin{bmatrix}
0.42 & 0.16 & 0.42 \\
0.16 & 0.42 & 0.42 \\
0.21 & 0.21 & 0.58
\end{bmatrix}
$$

---

### Step 5. Weighted Sum of Values

Now we multiply $AV$:

$$
V = X =
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}
$$

So:

$$
AV =
\begin{bmatrix}
0.42 & 0.16 & 0.42 \\
0.16 & 0.42 & 0.42 \\
0.21 & 0.21 & 0.58
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}
=
\begin{bmatrix}
0.84 & 0.58 \\
0.58 & 0.84 \\
0.79 & 0.79
\end{bmatrix}
$$

---

### Step 6. Interpretation

- The output for "I" is $[0.84, 0.58]$ ‚Üí it has picked up information from "AI" and a little from "love."
- The output for "love" is $[0.58, 0.84]$ ‚Üí similar structure, but weighted more toward itself and "AI."
- The output for "AI" is $[0.79, 0.79]$ ‚Üí a strong blend of everyone.

**And that‚Äôs it**: all we did was matrix multiply, but we ended up with each token enriched by context from every other token.

---

üí° This toy walkthrough could go directly in your blog. It answers your ‚Äúwhy matmul?‚Äù question by showing that:

- $QK^T$ is literally _all pairwise similarities in one shot_.
- Multiplying by $V$ lets us _blend representations_ based on those similarities.

---

üëâ Do you want me to also draft a **visual (matrix heatmap + arrows)** version of this so your readers see the numbers turning into contextualized vectors step by step?
