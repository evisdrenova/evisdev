---
title: "Why MatMul is the Beating Heart of Transformers"
date: "2025-10-04"
publish: false
---

Over the past few months, I've built a few LLMs from scratch, namely GPT-rs and llama2.rs. These are full re-writes of GPT-2 from Python to Rust and Llama2 from C to Rust. At this point, I feel reasonably comfortable with the transformer architecture and how it works.

But I often find myself asking, _why_ does it work? Why is that matrix multiplying two matrices together is the _right_ operation and how does that lead to a system like ChatGPT giving us a coherent response to our prompts?

It's one thing to implement `matmul` it's another to deeply understand why it works.

Hopefully this blog helps you to build that intuition.

# What is Matrix Multiplication?

One of the core operations within the layers of a neural net is matrix multiplication or `matmul` as it's often referred to. Matrix multiplication is a way to linearly combine two vectors in order to create a new vector.

Let's break down exactly what the means.

A **vector** is a direction in space that has a magnitude. We use the bracket notation to represent the vector in the form `[x,y]`. For example, the vector, $v_1$, at `[0,1]` can be drawn on a graph like:

![ve](/vec1.png)

The **direction** of the vector is the direction of the arrowhead and usually is written as the angle the vector makes with regard to the positive x-axis or a compass direction. Since our vector points straight up, it has a direction of 90 degrees.

The **magnitude** of the vector is the numerical size or length of the vector. Our vector has a magnitude of one since it starts at `[0,0]` and ends at `[0,1]`.

I can then add another vector, $v_2$, here at `[1,0]` shown in orange:

![ve](/vec2.png)

In order to linearly combine these vectors, we simply take the weighted sum of the vectors. We can represent the linear combination of two vectors with the equation:

$$
a v_1 + b v_2
$$

Where $a$ and $b$ are weights (or scalars) between `[0,1]` that we use to _scale_ the vectors. If we randomly set these weights to $a = 0.5$, $b = 0.8$ and plug them into our equation, we get:

$$
a v_1 + b v_2
$$

$$
0.5 \cdot [1,0] + 0.8 \cdot [0,1]
$$

We can now multiply each element in the $v_1$ `[1,0]` vector by $0.5$:

$$
a \cdot v_1 = 0.5 \cdot [1, 0] = [0.5 \cdot 1, 0.5 \cdot 0] = [0.5, 0]
$$

So we took our original `[1,0]` vector and then multiplied it by the scalar $0.5$ and got `[0.5,0]`. We just shrunk our vector or reduced it's magnitude by 50%!

We can do the same thing for the $v_2$ `[0,1]` vector and get:

$$
a \cdot v_2 = 0.8 \cdot [0, 1] = [0.8 \cdot 0, 0.8 \cdot 1] = [0, 0.8]
$$

This time, we only shrunk our vector by 20%.

Lastly, we can add these two vectors by adding the `x` position of each vector and then the `y` position of each vector like this:

$$
\begin{align}
a v_1 + b v_2 &= [0.5, 0] + [0, 0.8]\\
&= [0.5 + 0, 0 + 0.8]\\
&= [0.5, 0.8]
\end{align}
$$

In case it's not clear, this is what we did:

![vec](/vecadds.png)

This creates a new vector that we can plot along side the original two vectors:

![vec](/res_vec.png)

So we've now taken the weighted sum of the two input vectors, $v_1$ and $v_2$ and created a new vector which is a combination of the two input vectors.

# From Geometry to Meaning

At first glance, this might seem like simple geometry, and it is! But here's where it gets interesting: what if these vectors don't represent directions in space, but instead represent meaning in some abstract space?

In a large language model like ChatGPT, every word is represented as a vector, typically with hundreds or thousands of dimensions instead of just 2. When the model sees the word "king", it's not just storing the letters k-i-n-g, it's storing a vector like [0.23, -0.45, 0.87, ...] that captures the meaning of "king" based on how it's used in language. It learns this meaning as it's being trained on every book, article and piece of data on the internet and ultiamtely spits out a vector like [0.23, -0.45, 0.87, ...].

Here's the key insight: just like we combined our simple 2D vectors to create a new vector, neural networks combine word vectors to create new representations that capture more complex meanings.

## A Concrete Example

Let's say we have vectors for:

"king" → [0.9, 0.1] (masculine, royalty)
"woman" → [0.1, 0.9] (feminine, person)

If we do the operation: king - man + woman, we're doing vector arithmetic that might give us something close to the vector for "queen"! This is because we're:

Taking the "royalty" concept from "king"
Removing the "masculine" aspect
Adding the "feminine" aspect

This is exactly the same weighted sum operation we just did, except now it's operating on semantic meaning rather than spatial directions.
