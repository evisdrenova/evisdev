---
title: "Building an autograd system in Rust"
date: "2025-11-08"
publish: false
---

I've been spending a lot of time training neural nets lately and fortunately or unfortunately, depending on which camp you fall on, python makes it very easy to train models.

Want to train a model? Just run `model.train()`.

Want to calculate your loss? Just run `loss = criterion(pred, target)`

Want to run gradient descent? Just run `loss.backward()`

Call `optimizer.step()` and boom - your model learns.

And yet… every time I write code like this, I can't help but feel like a sense of dissatisfaction. As if I don't actually appreciate how easy Pytorch makes this insanely complicated process. I know there’s an enormous amount of machinery happening under the hood from computational graphs, chained gradients, parameter updates and more, and yet it's all wrapped up in nice little function calls.

So let's peel back the abstraction and implement it from scratch in order to really gain an appreciation of the complexity that pytorch saves us from.

This blog walks through building a very basic autograd system in rust called `nanograd`. The goal isn’t performance or completeness, it’s mainly intuition. I wanted a clearer mental model of how gradients flow, how computation graphs form, and how optimizers update parameters.

If you’ve ever wondered what really happens beneath loss.backward(), this is a great place to start.

Let’s dive in.

# What is autograd?

If you're not familiar with autograd then let's take a minute to walk through it. If you are familiar with it and how it works in Pytorch, then you can probably skip this section.

In the Pytorch world, autograd is Pytorch's automatic differentiation engine. What does that mean?

When we train neural nets, there are two main steps:

![loss-landscape](/optimizers/back.png)

1. **Forward pass (propagation)**: The neural net is fed input data and it makes predictions using its current weights. In the image above, the forward pass moves "through" the model from left (input) to right (output). At the end, we compute a loss that measures how wrong our predictions were.

2. **Backward pass (propagation)**: The neural net adjusts its parameters based on the loss. It does this by traversing backwards through the network (opposite of the forward pass) and calculating the gradient of the loss with respect to each parameter using the chain rule. These gradients tell us how to update each parameter to reduce the loss.

The **"automatic"** part means PyTorch computes these gradients for you automatically. You don't need to manually derive and code up the derivatives for each operation in your network.

The **"differentiation"** part refers to computing derivatives (gradients). Specifically, autograd calculates ∂Loss/∂(every parameter) - the gradient of the loss with respect to each of the millions (or billions) of parameters in your model.

Without autograd, you'd need to manually compute and code the derivative for every operation in your network using calculus. With autograd, you just call `loss.backward()` and PyTorch figures it all out using the chain rule.

# Autograd main comoponents

We can break down an autograd system into 3 main components.

## 1. Computational Graph

1. **Computational graph**: the computational graph is a directed acyclic graph (DAG) that stores the operations of the autograd process.

- **Nodes**: Represent tensors (both inputs and intermediate results)
- **Edges**: Represent operations that transform one tensor into another
- **Leaves**: Input tensors and parameters (where gradients will accumulate)
- **Root**: The final loss value

![comp](/nanograd/comp.png)

During the forward pass, autograd does two things simultaneously:

- Runs the requested operation to compute the resulting tensor
- Records the operation's gradient function in the DAG

During the backward pass:

- Computes the gradients from each operation's gradient function (`.grad_fn`)
- Accumulates them in each tensor's `.grad` attribute
- Uses the chain rule to propagate gradients all the way back to the leaf tensors

By tracing this graph from root to leaves, you can automatically compute the gradients using the chain rule.

## 2. Tensor Wrapper

![tens](/nanograd/tens.png)

The tensor wrapper is the core data structure that makes automatic differentiation possible. It wraps the raw tensor value with metadata needed for gradient computation.

- **Store the data**: The actual tensor
- **Track gradient requirements**: A flag (`requires_grad`) indicating whether this tensor needs gradients computed for it
- **Store accumulated gradients**: A `.grad` field that accumulates gradients during the backward pass
- **Link to computation history**: A reference to the operation (`grad_fn`) that created this tensor
- **Maintain connections**: References to parent tensors

The tensor wrapper is what allows PyTorch to say "this tensor came from multiplying x by 2, so I know how to compute gradients for it."

## 3. Gradient Functions

![tens](/nanograd/grad.png)

Gradient functions define how to compute derivatives for each operation. Every operation in your neural network (add, multiply, matrix multiply, ReLU, etc.) needs a corresponding gradient function.

- **Store operation-specific context**: Information needed to compute gradients (e.g., the input values, shapes, etc.)
- **Implement the derivative**: The mathematical formula for how this operation's output gradient relates to its input gradients
- **Apply the chain rule**: Take the gradient flowing backwards and compute gradients for each input

Each gradient function typically needs:

1. **Forward context**: Data saved during the forward pass (needed for backward computation)
2. **Backward method**: Given the gradient of the loss with respect to the output, compute the gradient with respect to each input

Let's look at the multiplication gradient function as an example:

$$
\textbf{Forward: } z = x \cdot y \text{ (save } x \text{ and } y \text{ for later)}
$$

$$
\textbf{Backward:}
$$

$$
\frac{\partial \text{Loss}}{\partial x} = \frac{\partial \text{Loss}}{\partial z} \times \frac{\partial z}{\partial x} = \frac{\partial \text{Loss}}{\partial z} \times y
$$

$$
\frac{\partial \text{Loss}}{\partial y} = \frac{\partial \text{Loss}}{\partial z} \times \frac{\partial z}{\partial y} = \frac{\partial \text{Loss}}{\partial z} \times x
$$

Notice how multiplication needs to save the input values (x and y) during the forward pass to compute gradients during the backward pass. This is why gradient functions need to store context.

Each operation in the computational graph has one of these gradient functions attached which forms the blueprint for the backward pass.

# Defining an autograd system in Rust

Now that we have a good foundation of how autograd works and it's main components, let's starting spec'ing out what we're going to build in rust.

Here is my plan:

1. Create the core type that will wrap the number and track it's computational history. This maps to the first and second components of the autograd system.
2. Implement a basic forward pass with addition and multiplication operations.
3. Implement the backward pass to compute gradients.
4. Add more operations (maybe)
5. Build an actual neural network using our nanograd building blocks
6. Training loop to put it all together and make sure it works.

I think this seems like a reasonable start. I'm also going to use Pytorch as a reference implementation to check out work along the way and make sure we're getting the same output so we know we're on the right track.

# Project set up

First things first, let's set up our project. I'm going to create a rust project using cargo:

`cargo init nanograd  && cd nanograd`

And then in another folder, the pytorch equivalent to check our work:

`uv init nanograd-py && cd nanograd-py`
`source .venv/bin/activate`
`uv add torch`

I could just group them into the same project with different sub directories, but I don't want to commit the python project. It's really just meant as a tool, the code has no functional use.

Okay, cool, we have our projects set up.

Let's get started with the first step: creating the core type that will wrap our tensors number and track it's computational history.

# Creating the core type

What does this need to store? Here's what I'm thinking:

- data (actual tensor)
- gradient (computed during backprop)
- backward_fn (function that knows how to compute gradients)
- references to parent values that created this value

Let's start with a couple of structs:

```rust
use std::sync::{Arc, Mutex};

struct TensorData {
    data: f64,
    grad: f64,
}

pub struct Tensor {
    inner: Arc<Mutex<TensorData>>,
}
```

We first define a struct called `TensorData` which will hold our data, the gradients and more fields that we'll add later. Then we wrap that in another struct called `Tensor` with a mutex. Why? Because we'll be updating the gradients during backprop so we need to make sure it's thread safe in case we decide to parallelize it.

Next we implement 3 functions:

- `new`: create a new Tensor
- `data`: return the data from the TensorData
- `grad`: returns the current gradient

```rust
impl Tensor {
    pub fn new(data: f64) -> Self {
        Tensor {
            inner: Arc::new(Mutex::new(TensorData { data, grad: 0.0 })),
        }
    }

    pub fn data(&self) -> f64 {
        self.inner.lock().unwrap().data
    }

    pub fn grad(&self) -> f64 {
        self.inner.lock().unwrap().grad
    }
}
```

Notice how we `lock().unwrap()` the TensorData struct so that we can safely access it in a multi-threaded manner.

Alright, we finally have some code down. It's simple but it's a start. Let's run it as well as our pytorch check and compare the output.

```rust
//in main.rs
use nanograd::tensor::Tensor;

fn main() {
    let t = Tensor::new(5.0);
    println!("data: {}", t.data());
    println!("grad: {}", t.grad());
}
```

This outputs:

```bash
[11/8/25 11:26:15] ~/code/projects/nanograd (main ✔) cargo run
   Compiling nanograd v0.1.0 (/Users/evisdrenova/code/projects/nanograd)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.29s
     Running `target/debug/nanograd`
data: 5
grad: 0
```

We're just simply instantiating a new `Tensor` struct and return the data and grad fields. Let's check pytorch:

```python
import torch

t = torch.tensor(5.0, requires_grad=True)
print(f"data: {t.item()}")
print(f"grad: {t.grad}")
```

```bash
(nanograd-py) [11/8/25 11:31:41] ~/code/projects/nanograd-py (main ✗) uv run main.py
data: 5.0
grad: None
```

They match up pretty well! One thing to note: pytorch uses "None" because it can distinguish between gradients that haven't been run yet ("None") and gradients that are equal to zero while our code just uses `0.0` for both. Down the line we can update it to support the same use-case as pytorch but for now they're essentially the same thing.

# Adding a computational graph

Next, let's add the ability to track where a tensor comes from.

To do that, we need to add two fields to our struct:

- `prev`: a list of the parent tensors
- `backward_fn`: how to compute gradients; similar to the third autograd component

Let's update our `TensorData` struct:

```rust
type BackwardFn = Box<dyn Fn() + Send>;


struct TensorData {
    data: f64,
    grad: f64,
    prev: Vec<Tensor>,
    backward_fn: Option<BackwardFn>,
}
```

`prev` is just a vector of `Tensor` and `backward_fn` is a `BackwardFn` that we defined above the struct and wrapped in an `Option` to make it an optional value.

Let's look at the `BackwardFn` type:

- `Fn()`: a function that takes in no arguments and doesn't return anything - it will capture what it needs from the surrounding scope
- `dyn`: the function will be **dynamic** because we don't know the exact type of function at compile time, so rust will figure it out at runtime.
- `Box<...>`: `dyn Fn()` is a trait, not a concrete type - its size is unknown at compile time. Box also puts it on the heap and gives us a fixed-size pointer. Without it, rust wouldn't know how much space to allocate for `TensorData`.
- `+ Send`: this is a trait bound meaning the function must be saqfe to send between threads. We need this because we're using `Arc` which allows sharing between threads.

Remember from the third section in the autograd components, different operations have different functions, so we have to define a generic function like `Fn()` and wrap it in a `Box<...>`, so we can get a fixed size pointer regardless of the size of the data.

You can think of it like a box containing any function that takes no args and is safe to share between threads.

Next, let's update our `new` function to add on these additional fields and initialize them.

```rust
    pub fn new(data: f64) -> Self {
        Tensor {
            inner: Arc::new(Mutex::new(TensorData {
                data,
                grad: 0.0,
                prev: Vec::new(),
                backward_fn: None,
            })),
        }
    }
```

Since this is a new Tensor, we can intiialize our `prev` field with a new vector and set our `backward_fn` to `None`. Why do we set the `backward_fn` to `None`?

If you we refer back to our computational graph diagram from above, leaf tensors are created with `new()` and don't have any backward functions because they're inputs. Computed tensors, the ones created by operations like add, multiply, etc., get their backward function set by that operation when its created.

So we actually need to add another constructor that will set the backward function when we create a new tensor as part of an operation. Luckily, it's pretty easy.

```rust
    fn from_op(data: f64, prev: Vec<Tensor>, backward_fn: BackwardFn) -> Self {
        Tensor {
            inner: Arc::new(Mutex::new(TensorData {
                data,
                grad: 0.0,
                prev,
                backward_fn: Some(backward_fn),
            })),
        }
    }
```

We just update the `Tensor` struct with the backward function that we pass in.

And with that we're done with the first step! We created the core data type to store our tensor data, gradients, parent history and backward function. We also added the ability to create leaf tensors and tensors from operations and a few getters; and made all of it thread safe.

Let's move on to the second step: Implement a basic forward pass with addition and multiplication operations.

# Creating a Forward Pass operation

We're going to add on the addition operation first. Just like we learned `c = a + b` in elementary school, we just add the data fields together like `c.data = a.data + b.data` for the forward pass.

The backward pass uses the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to calculate the gradient of `a` and `b` like:

- `∂Loss/∂a = ∂Loss/∂c × 1`
- `∂Loss/∂b = ∂Loss/∂c × 1`

It's critical that we understand how the chain rule works and the intuition behind it. The chain rule tells us how to compute gradients when a function is composed i.e. when a function is made up of multiple sub-functions. In our example below, the Loss is a composed function because, first, you have to add `a` + `b` (function 1) and then square it (function 2) to get the loss.

Let's say that we have:

```
a = 2.0
b = 3.0
c = a + b = 5.0
Loss = c² = 25.0
```

We want to know: "If I change `a` by some amount, how much does the `Loss` change?"

That's what `∂Loss/∂a` means, the gradient of Loss with respect to a.

The Chain Rule says:

`∂Loss/∂a = ∂Loss/∂c × ∂c/∂a`

Said differently: "How `Loss` changes w.r.t. `a` = (How `Loss` changes w.r.t. `c`) × (How `c` changes w.r.t. `a`)".

You can see the "backward" path here from Loss -> c -> a. Just the reverse path of our composed function.

Breaking it down, we can calculate the first term, `∂Loss/∂c`:

- `Loss = c²`
- `∂Loss/∂c = 2c = 2 × 5 = 10` (we take the derivative of the loss to make it `2c`)

This is the gradient flowing backward from the loss. In our computational graph, this is flowing from the root to the leaf nodes.

The second term: `∂c/∂a`, is calculated by:

- `c = a + b`
- `∂c/∂a = 1` (if you increase `a` by 1, `c` increases by 1)

This is specific to the addition operation. In other operations, this will change.

Combining them:

```∂Loss/∂a = ∂Loss/∂c × ∂c/∂a
         = 10 × 1
         = 10
```

Graphically:

![tens](/nanograd/graph.png)

You can imagine a slider on `a` which as you move it from right to left, updates the curve and the slope of the tangent line (our gradient).

If we update `a` to 5, then our loss goes from `25` to `64`. And our loss gradient with respect to `a` goes from `10` to `16`.

We can repeat the same process for `b`:

```
∂c/∂b = 1  (if you increase b by 1, c increases by 1)
∂Loss/∂b = ∂Loss/∂c × ∂c/∂b = 10 × 1 = 10
```
