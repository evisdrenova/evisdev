---
title: "Building an autograd system in Rust"
date: "2025-11-08"
publish: false
---

I've been spending a lot of time training neural nets lately and fortunately or unfortunately, depending on which camp you fall on, python makes it very easy to train models.

Want to train a model? Just run `model.train()`.

Want to calculate your loss? Just run `loss = criterion(pred, target)`

Want to run gradient descent? Just run `loss.backward()`

Call `optimizer.step()` and boom - your model learns.

And yet… every time I write code like this, I can't help but feel like a sense of dissatisfaction. As if I don't actually appreciate how easy Pytorch makes this insanely complicated process. I know there’s an enormous amount of machinery happening under the hood from computational graphs, chained gradients, parameter updates and more, and yet it's all wrapped up in nice little function calls.

So let's peel back the abstraction and implement it from scratch in order to really gain an appreciation of the complexity that pytorch saves us from.

This blog walks through building a very basic autograd system in rust called `nanograd`. The goal isn’t performance or completeness, it’s mainly intuition. I wanted a clearer mental model of how gradients flow, how computation graphs form, and how optimizers update parameters.

If you’ve ever wondered what really happens beneath loss.backward(), this is a great place to start.

Let’s dive in.

# What is autograd?

If you're not familiar with autograd then let's take a minute to walk through it. If you are familiar with it and how it works in Pytorch, then you can probably skip this section.

In the Pytorch world, autograd is Pytorch's automatic differentiation engine. What does that mean?

When we train neural nets, there are two main steps:

![loss-landscape](/optimizers/back.png)

1. **Forward pass (propagation)**: The neural net is fed input data and it makes predictions using its current weights. In the image above, the forward pass moves "through" the model from left (input) to right (output). At the end, we compute a loss that measures how wrong our predictions were.

2. **Backward pass (propagation)**: The neural net adjusts its parameters based on the loss. It does this by traversing backwards through the network (opposite of the forward pass) and calculating the gradient of the loss with respect to each parameter using the chain rule. These gradients tell us how to update each parameter to reduce the loss.

The **"automatic"** part means PyTorch computes these gradients for you automatically. You don't need to manually derive and code up the derivatives for each operation in your network.

The **"differentiation"** part refers to computing derivatives (gradients). Specifically, autograd calculates ∂Loss/∂(every parameter) - the gradient of the loss with respect to each of the millions (or billions) of parameters in your model.

Without autograd, you'd need to manually compute and code the derivative for every operation in your network using calculus. With autograd, you just call `loss.backward()` and PyTorch figures it all out using the chain rule.

# Autograd main comoponents

We can break down an autograd system into 3 main components.

## 1. Computational Graph

1. **Computational graph**: the computational graph is a directed acyclic graph (DAG) that stores the operations of the autograd process.

- **Nodes**: Represent tensors (both inputs and intermediate results)
- **Edges**: Represent operations that transform one tensor into another
- **Leaves**: Input tensors and parameters (where gradients will accumulate)
- **Root**: The final loss value (where backward pass starts)

![comp](/nanograd/comp.png)

During the forward pass, autograd does two things simultaneously:

- Runs the requested operation to compute the resulting tensor
- Records the operation's gradient function in the DAG (stores how to compute derivatives for this operation)

During the backward pass:

- Computes the gradients from each operation's gradient function (`.grad_fn`)
- Accumulates them in each tensor's `.grad` attribute
- Uses the chain rule to propagate gradients all the way back to the leaf tensors

By tracing this graph from root to leaves, you can automatically compute the gradients using the chain rule.

## 2. Tensor Wrapper

![tens](/nanograd/tens.png)

The tensor wrapper is the core data structure that makes automatic differentiation possible. It wraps the raw numerical data (the actual tensor values) with metadata needed for gradient computation.

**Key responsibilities:**

- **Store the data**: The actual numerical values (could be a scalar, vector, matrix, or higher-dimensional array)
- **Track gradient requirements**: A flag (`requires_grad`) indicating whether this tensor needs gradients computed for it
- **Store accumulated gradients**: A `.grad` field that accumulates gradients during the backward pass
- **Link to computation history**: A reference to the operation (`grad_fn`) that created this tensor (if any)
- **Maintain connections**: References to parent tensors (inputs to the operation that created this tensor)

The tensor wrapper is what allows PyTorch to say "this tensor came from multiplying x by 2, so I know how to compute gradients for it."

## 3. Gradient Functions (Backward Functions)

![tens](/nanograd/gradfuncs.png)

Gradient functions define **how to compute derivatives** for each operation. Every operation in your neural network (add, multiply, matrix multiply, ReLU, etc.) needs a corresponding gradient function.

**Key responsibilities:**

- **Store operation-specific context**: Information needed to compute gradients (e.g., the input values, shapes, etc.)
- **Implement the derivative**: The mathematical formula for how this operation's output gradient relates to its input gradients
- **Apply the chain rule**: Take the gradient flowing backwards and compute gradients for each input

**Structure:**
Each gradient function typically needs:

1. **Forward context**: Data saved during the forward pass (needed for backward computation)
2. **Backward method**: Given the gradient of the loss with respect to the output, compute the gradient with respect to each input

**Example - Addition:**

```
Forward: z = x + y
Backward:
  ∂Loss/∂x = ∂Loss/∂z × ∂z/∂x = ∂Loss/∂z × 1 = ∂Loss/∂z
  ∂Loss/∂y = ∂Loss/∂z × ∂z/∂y = ∂Loss/∂z × 1 = ∂Loss/∂z
```

**Example - Multiplication:**

```
Forward: z = x * y (save x and y for later)
Backward:
  ∂Loss/∂x = ∂Loss/∂z × ∂z/∂x = ∂Loss/∂z × y
  ∂Loss/∂y = ∂Loss/∂z × ∂z/∂y = ∂Loss/∂z × x
```

Notice how multiplication needs to save the input values (x and y) during the forward pass to compute gradients during the backward pass. This is why gradient functions need to store context.

**Common gradient functions you'll need:**

- `AddBackward`: Gradient for addition
- `MulBackward`: Gradient for element-wise multiplication
- `MatMulBackward`: Gradient for matrix multiplication
- `ReluBackward`: Gradient for ReLU activation
- `SumBackward`: Gradient for sum reduction
- `PowBackward`: Gradient for power operations

Each operation in your computational graph has one of these gradient functions attached, forming the blueprint for the backward pass.

# Defining an autograd system in rust
