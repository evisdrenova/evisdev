---
title: "Are Ambient Agents Next?"
date: "2025-06-24"
---

Today, most AI agents are reactive.

Agents are invoked by some process (either manual or programmatic) to complete a very well-defined task and once that task is complete, the agent effectively shuts down and waits for the next invocation.

There are a lot of use-cases where this is the right interaction model but I can't help but wonder if there is another model, a more proactive model that hasn't really been explored yet? I mean the promise of agents is to 'act' like an employee and if you had to give your employee discrete directions for every task - it would be pretty annoying.

As you would with a human employee, you want to give an agent a high level objective function and then have it constantly work to optimize that objectve function. Now, in the real world, it likely can't just run on it's own in perpetuity and will need some direction from the user.

Ideally, over time, that agent 'learns' how you work and when to ask for help. Through this interaction model, in a way, you're using reinforcement learning to guide the agent towards that outcomes and behaviors that you want. This is effectively how humans work.

But what would this look like in practice? Let's compare and contrast a junior developer with an ambient agent.

## The human workflow:

**Objective function:**
You are a junior developer. Your role is to contribute to the codebase, learn from senior developers, and help ship features. You should write clean code, follow team conventions, and ask for help when stuck.

**Daily operations:**

- Check Slack/email for updates
- Pull latest code changes
- Work on assigned tickets in JIRA
- Ask questions when blocked
- Submit PRs for review
- Respond to code review feedback
- Attend standups and meetings

**When they need help:**

- Stuck on a technical problem for >30 minutes
- Unclear about requirements
- Not sure about architectural decisions
- Code review feedback they don't understand

## The ambient agent workflow:

**Objective function:**  
You are an ambient coding agent. Your role is to continuously monitor the codebase, identify improvement opportunities, and execute tasks that move the project forward. You should follow established patterns, ask for guidance on ambiguous decisions, and learn from feedback.

**Continuous operations:**

- Monitor repository for new issues/PRs
- Scan codebase for potential improvements (dead code, missing tests, documentation gaps)
- Automatically handle routine tasks (dependency updates, formatting fixes)
- Track project metrics and flag concerning trends
- Prepare draft PRs for human review

**When it needs help:**

- Ambiguous requirements in tickets
- Multiple valid implementation approaches
- Breaking changes that affect other teams
- Failed attempts after several iterations

## The key difference: Learning over time

Here's where it gets interesting. A junior developer learns through:

- Code reviews ("don't use var, use const")
- Verbal feedback ("we prefer smaller functions")
- Observing senior developer patterns
- Team retrospectives

An ambient agent learns through reinforcement:

- Positive signals: PR approved quickly, no follow-up changes requested
- Negative signals: PR requires major revisions, human overrides agent decisions
- Pattern recognition: "When I do X in this context, I get positive feedback"
- Preference modeling: Learning that this human prefers verbose comments while that one likes minimal code

Over time, the agent builds a model of how _you specifically_ like to work. Maybe you're cool with the agent autonomously fixing linting errors but always want to review dependency updates. Maybe you want the agent to be more aggressive about refactoring on Fridays when you have more time for review.

## The interaction model

The magic is in the handoff between human and agent. Unlike a junior developer who might interrupt you with questions, the ambient agent operates more like having a really good pair programmer who's always working in the background.

It might surface a notification: "I found 12 functions without tests in the payments module. Should I draft some basic test coverage?"

Or: "The API response time for user queries has increased 40% since last week. I can investigate the recent changes - should I proceed?"

The human stays in control but doesn't have to micromanage. The agent gets better at predicting when to act autonomously versus when to ask. It's like having a junior developer who gradually requires less supervision.

This is fundamentally different from today's reactive agents. Instead of "hey agent, do this specific thing," it's more like "here's what good looks like, now go make it happen while I focus on the hard problems."

Will this actually work? Hell if I know. But it feels like the next logical step in making AI agents actually useful rather than just impressive demos.
